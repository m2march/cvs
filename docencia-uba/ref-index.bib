@article{steedman1977perception,
author = {Mark J Steedman},
title ={The Perception of Musical Rhythm and Metre},
journal = {Perception},
volume = {6},
number = {5},
pages = {555-569},
year = {1977},
doi = {10.1068/p060555},
    note ={PMID: 593789},

URL = { 
        https://doi.org/10.1068/p060555
    
},
eprint = { 
        https://doi.org/10.1068/p060555
    
}
,
    abstract = { The occurrence of relatively long notes, and the repetition of melodic phrases are important cues to the metre, or regular beat, of a piece of music. A model of how people use this information to infer the metre of unaccompanied melodies is described here. The model is in the form of a computer program, and involves a definition of melodic repetition which encompasses repetitions that include certain kinds of variation. The program has been applied to the task of analysing the metric structure of the forty-eight fugue subjects of the Well-Tempered Clavier by J S Bach. The program is discussed in relation to other models both of musical understanding and of sequential concept learning. }
}

@book{huron2006sweet,
  title={Sweet anticipation: Music and the psychology of expectation},
  author={Huron, David Brian},
  year={2006},
  publisher={MIT press}
}

@article{burns1978categorical,
  title={Categorical perception—phenomenon or epiphenomenon: Evidence from experiments in the perception of melodic musical intervals},
  author={Burns, Edward M and Ward, W Dixon},
  journal={The Journal of the Acoustical Society of America},
  volume={63},
  number={2},
  pages={456--468},
  year={1978},
  publisher={Acoustical Society of America}
}

@article{goto2001audio,
  title={An audio-based real-time beat tracking system for music with or without drum-sounds},
  author={Goto, Masataka},
  journal={Journal of New Music Research},
  volume={30},
  number={2},
  pages={159--171},
  year={2001},
  publisher={Taylor \& Francis}
}

@inproceedings{allen1990tracking,
  title={Tracking musical beats in real time},
  author={Allen, Paul E and Dannenberg, Roger B},
  booktitle={Proceedings of the 1990 International Computer Music Conference},
  volume={140143},
  year={1990}
}

@article{clarke1987categorical,
  title={Categorical rhythm perception: an ecological perspective},
  author={Clarke, Eric F},
  journal={Action and perception in rhythm and music},
  volume={55},
  pages={19--33},
  year={1987},
  publisher={Royal Swedish Academy of Music Stockholm,, Sweden}
}

@book{schloss1985automatic,
  title={On the automatic transcription of percussive music: from acoustic signal to high-level analysis},
  author={Schloss, Walter Andrew},
  number={27},
  year={1985},
  publisher={Stanford University}
}

@book{michon1967timing,
  title={Timing in temporal tracking},
  author={Michon, John Albertus and Instituut voor Zintuigfysiologie RVO-TNO (Soesterberg, Pays-Bas).},
  year={1967},
  publisher={Institute for Perception RVO-TNO Soesterberg, The Netherlands}
}

@article{essens1985metrical,
  title={Metrical and nonmetrical representations of temporal patterns},
  author={Essens, Peter J and Povel, Drik-Jan},
  journal={Perception \& Psychophysics},
  volume={37},
  number={1},
  pages={1--7},
  year={1985},
  publisher={Springer}
}

@article{povel1985perception,
  title={Perception of temporal patterns},
  author={Povel, Dirk-Jan and Essens, Peter},
  journal={Music Perception: An Interdisciplinary Journal},
  volume={2},
  number={4},
  pages={411--440},
  year={1985},
  publisher={University of California Press Journals}
}

@article{povel1981accents,
  title={Accents in equitone sequences},
  author={Povel, Dirk-Jan and Okkerman, Hans},
  journal={Perception \& Psychophysics},
  volume={30},
  number={6},
  pages={565--572},
  year={1981},
  publisher={Springer}
}

@book{lerdahl1985generative,
  title={A generative theory of tonal music},
  author={Lerdahl, Fred and Jackendoff, Ray},
  year={1985},
  publisher={MIT press}
}

@book{temperley2001cognition,
  title={The cognition of basic musical structures},
  author={Temperley, David},
  year={2001},
  publisher={MIT press}
}

@book{temperley2007music,
  title={Music and probability},
  author={Temperley, David},
  year={2007},
  publisher={The MIT Press}
}

@phdthesis{rosenthal1992machine,
  title={Machine rhythm--computer emulation of human rhythm perception},
  author={Rosenthal, David Felix},
  year={1992},
  school={Massachusetts Institute of Technology}
}

@book{martineau2008elements,
  title={The Elements of Music: Melody, Rhythm, and Harmony},
  author={Martineau, J.},
  isbn={9780802716828},
  series={Wooden Books},
  url={https://books.google.com.ar/books?id=fyKdLgAACAAJ},
  year={2008},
  publisher={Walker}
}

@book{kostka1995workbook,
  title={Workbook for Tonal harmony, with an introduction to twentieth-century music},
  author={Kostka, S.M. and Payne, D. and Schindler, A.},
  lccn={94232816},
  year={1995},
  publisher={McGraw-Hill}
}

@article{hannon2005,
    title = "Infants use meter to categorize rhythms and melodies: Implications
        for musical structure learning ",
    journal = "Cognitive Psychology ",
    volume = "50",
    number = "4",
    pages = "354 - 377",
    year = "2005",
    note = "",
    issn = "0010-0285",
    doi = "http://dx.doi.org/10.1016/j.cogpsych.2004.09.003",
    url =
        "http://www.sciencedirect.com/science/article/pii/S001002850400074X",
    author = "Erin E. Hannon and Scott P. Johnson",
    keywords = "Development",
    keywords = "Pattern perception",
    keywords = "Music and speech",
    keywords = "Rhythm and meter",
    keywords = "Statistical learning",
    keywords = "Bootstrapping "
}

@article{large1995reduced,
    title={Reduced memory representations for music},
    author={Large, Edward W and Palm{\.e}r, Caroline and Pollack, Jordan
        B},
    journal={Cognitive science},
    volume={19},
    number={1},
    pages={53--96},
    year={1995},
    publisher={Wiley Online Library}
}

@article{large1999dynamics,
  title={The dynamics of attending: How people track time-varying events.},
  author={Large, Edward W and Jones, Mari Riess},
  journal={Psychological review},
  volume={106},
  number={1},
  pages={119},
  year={1999},
  publisher={American Psychological Association}
}

@article{large2002perceiving,
  title={Perceiving temporal regularity in music},
  author={Large, Edward W and Palmer, Caroline},
  journal={Cognitive Science},
  volume={26},
  number={1},
  pages={1--37},
  year={2002},
  publisher={Elsevier}
}

@article{nollmathematics,
  title={Mathematics and Computation in Music},
  author={Noll, Timour Klouche Thomas},
  publisher={Springer},
  year={2007}
}

@article{pressing1997spectral,
  title={Spectral properties of human cognition and skill},
  author={Pressing, Jeff and Jolley-Rogers, Garry},
  journal={Biological cybernetics},
  volume={76},
  number={5},
  pages={339--347},
  year={1997},
  publisher={Springer}
}

@incollection{chen2003long,
  title={Long range dependence in human sensorimotor coordination},
  author={Chen, Yanqing and Ding, Mingzhou and Kelso, JA Scott},
  booktitle={Processes with long-range correlations},
  pages={309--323},
  year={2003},
  publisher={Springer}
}

@article{kadota2004time,
  title={Time-series pattern changes related to movement rate in synchronized human tapping},
  author={Kadota, Hiroshi and Kudo, Kazutoshi and Ohtsuki, Tatsuyuki},
  journal={Neuroscience letters},
  volume={370},
  number={2},
  pages={97--101},
  year={2004},
  publisher={Elsevier}
}

@article{cornelis2013evaluation,
  title={Evaluation and Recommendation of Pulse and Tempo Annotation in Ethnic Music},
  author={Cornelis, Olmo and Six, Joren and Holzapfel, Andre and Leman, Marc},
  journal={Journal of New Music Research},
  volume={42},
  number={2},
  pages={131--149},
  year={2013},
  publisher={Taylor \& Francis}
}

@misc{website:music21,
    author = "Michael Scott Cuthbert",
    title = "Music21",
    organization = "MIT",
    url = "http://web.mit.edu/music21/"
}

@misc{website:mirex,
    title = "Mirex",
    url = "http://www.music-ir.org/mirex/wiki/MIREX_HOME"
}

@misc{website:beatroot,
    title = "BeatRoot",
    url = "https://code.soundsoftware.ac.uk/projects/beatroot/repository"
}

@inproceedings{bock2016madmom,
  title={Madmom: A new python audio and music signal processing library},
  author={B{\"o}ck, Sebastian and Korzeniowski, Filip and Schl{\"u}ter, Jan and Krebs, Florian and Widmer, Gerhard},
  booktitle={Proceedings of the 24th ACM international conference on Multimedia},
  pages={1174--1178},
  year={2016},
  organization={ACM}
}

@article{repp2013sensorimotor,
  title={Sensorimotor synchronization: a review of recent research (2006--2012)},
  author={Repp, Bruno H and Su, Yi-Huang},
  journal={Psychonomic bulletin \& review},
  volume={20},
  number={3},
  pages={403--452},
  year={2013},
  publisher={Springer}
}

@article{repp2006musical,
  title={Musical synchronization},
  author={Repp, Bruno Hermann},
  journal={Music, motor control, and the brain},
  pages={55--76},
  year={2006},
  publisher={Oxford University Press Oxford}
}

@article{repp2003rate,
  title={Rate limits in sensorimotor synchronization with auditory and visual sequences: The synchronization threshold and the benefits and costs of interval subdivision},
  author={Repp, Bruno H},
  journal={Journal of motor behavior},
  volume={35},
  number={4},
  pages={355--370},
  year={2003},
  publisher={Taylor \& Francis}
}

@article{repp2010self,
  title={Self-generated interval subdivision reduces variability of synchronization with a very slow metronome},
  author={Repp, Bruno H},
  year={2010},
  publisher={JSTOR}
}

@article{repp2010musicianship,
title = {Sensorimotor synchronization and perception of timing: Effects of music training and task experience},
journal = {Human Movement Science},
volume = {29},
number = {2},
pages = {200-213},
year = {2010},
issn = {0167-9457},
doi = {https://doi.org/10.1016/j.humov.2009.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167945709000943},
author = {Bruno H. Repp},
keywords = {Synchronization, Tapping, Phase correction, Timing, Music training},
abstract = {To assess individual differences in basic synchronization skills and in perceptual sensitivity to timing deviations, brief tests made up of isochronous auditory sequences containing phase shifts or tempo changes were administered to 31 college students (most of them with little or no music training) and nine highly trained musicians (graduate students of music performance). Musicians showed smaller asynchronies, lower tapping variability, and greater perceptual sensitivity than college students, on average. They also showed faster phase correction following a tempo change in the pacing sequence. Unexpectedly, however, phase correction following a simple phase shift was unusually quick in both groups, especially in college students. It emerged that some of the musicians, who had previous experience with laboratory synchronization tasks, showed a much slower corrective response to phase shifts than did the other musicians. When these others were retested after having gained some task experience, their phase correction was slower than previously. These results show (1) that instantaneous phase correction in response to phase perturbations is more common than was previously believed, and suggest that (2) gradual phase correction is not a shortcoming but reflects a reduction in the strength of sensorimotor coupling afforded by practice.}
}
@misc{website:melisma,
  title={The Melisma Music Analyzer},
  author={Sleator, Daniel and Temperley, David},
  url={www.link.cs.cmu.edu/music-analysis},
  year={2001}
}

@article{honing2002structure,
    title={Structure and interpretation of rhythm and timing},
    author={Honing, Henkjan},
    journal={Tijdschrift voor Muziektheorie},
    volume={7},
    number={3},
    pages={227--232},
    year={2002}
}

@article{honing2012without,
  title={Without it no music: beat induction as a fundamental musical trait},
  author={Honing, Henkjan},
  journal={Annals of the New York Academy of Sciences},
  volume={1252},
  number={1},
  pages={85--91},
  year={2012},
  publisher={Wiley Online Library}
}

@article{kessler1984balinese,
     jstor_articletype = {research-article},
     title = {tonal schemata in the perception of music in bali and in the west},
     author = {kessler, edward j. and hansen, christa and shepard, roger n.},
     journal = {music perception: an interdisciplinary journal},
     jstor_issuetitle = {},
     volume = {2},
     number = {2},
     jstor_formatteddate = {winter, 1984},
     pages = {pp. 131-165},
     url = {http://www.jstor.org/stable/40285289},
     issn = {07307829},
     abstract = {krumhansl and shepard's probe-tone method, in which listeners rate the musical relatedness of probe tones to preceding musical contexts, was adapted for a cross-cultural comparison of the perception of western and balinese melodies by both western and balinese listeners. half of the balinese listeners were remote villagers with no previous exposure to the diatonic scales or music of the west, and the western listeners were unfamiliar with the pelog and slendro scales and the music of bali. the balinese and western listeners used similar response strategies, but tended to demonstrate an internalization of tonal schemata most often in response to music of their own culture.},
     language = {english},
     year = {1984},
     publisher = {university of california press},
     copyright = {copyright © 1984 university of california press},
}

@misc{website:tap-playlist,
    title = {Lista de videos de Tap Dance},
    url = {http://www.youtube.com/playlist?list=PLom_QC7FwCdijrkubclFJP5-TGcRyGSwC}
}

@article{meyer1957meaning,
    title={Meaning in music and information theory},
    author={Meyer, Leonard B},
    journal={Journal of Aesthetics and Art Criticism},
    pages={412--424},
    year={1957},
    publisher={JSTOR}
}

@article{marr1976understanding,
    author = {Marr, D.C. and Poggio, Tomaso},
    year = {2004},
    month = {10},
    pages = {},
    title = {From Understanding Computation to Understanding Neural Circuitry},
    volume = {15},
    journal = {Neurosciences Research Program Bulletin}
}

@article{dixon2001automatic,
    title={Automatic extraction of tempo and beat from expressive
    performances},
    author={Dixon, Simon},
    journal={Journal of New Music Research},
    volume={30},
    number={1},
    pages={39--58},
    year={2001},
    publisher={Taylor \& Francis}
}

@book{welsh1996african,
    title={African Dance: An Artistic, Historical, and Philosophical
    Inquiry},
    author={Welsh-Asante, K.},
    isbn={9780865431973},
    lccn={94041598},
    url={https://books.google.com.ar/books?id=nnsWhJGkXo8C},
    year={1996},
    publisher={Africa World Press}
}

@article{vuust2014rhythmic,
AUTHOR={Vuust, Peter and Witek, Maria A. G.},   
TITLE={Rhythmic complexity and predictive coding: a novel approach to modeling rhythm and meter perception in music},      
JOURNAL={Frontiers in Psychology},      
VOLUME={5},      
PAGES={1111},     
YEAR={2014},      
URL={https://www.frontiersin.org/article/10.3389/fpsyg.2014.01111},       
DOI={10.3389/fpsyg.2014.01111},      
ISSN={1664-1078},   
ABSTRACT={Musical rhythm, consisting of apparently abstract intervals of accented temporal events, has a remarkable capacity to move our minds and bodies. How does the cognitive system enable our experiences of rhythmically complex music? In this paper, we describe some common forms of rhythmic complexity in music and propose the theory of predictive coding (PC) as a framework for understanding how rhythm and rhythmic complexity are processed in the brain. We also consider why we feel so compelled by rhythmic tension in music. First, we consider theories of rhythm and meter perception, which provide hierarchical and computational approaches to modeling. Second, we present the theory of PC, which posits a hierarchical organization of brain responses reflecting fundamental, survival-related mechanisms associated with predicting future events. According to this theory, perception and learning is manifested through the brain’s Bayesian minimization of the error between the input to the brain and the brain’s prior expectations. Third, we develop a PC model of musical rhythm, in which rhythm perception is conceptualized as an interaction between what is heard (“rhythm”) and the brain’s anticipatory structuring of music (“meter”). Finally, we review empirical studies of the neural and behavioral effects of syncopation, polyrhythm and groove, and propose how these studies can be seen as special cases of the PC theory. We argue that musical rhythm exploits the brain’s general principles of prediction and propose that pleasure and desire for sensorimotor synchronization from musical rhythm may be a result of such mechanisms.}
}

@article{temperley2012computational,
    title={Computational models of music cognition},
    author={Temperley, David},
    journal={The psychology of music},
    pages={327--368},
    year={2012}
}

@article{large1994resonance,
  title={Resonance and the perception of musical meter},
  author={Large, Edward W and Kolen, John F},
  journal={Connection science},
  volume={6},
  number={2-3},
  pages={177--208},
  year={1994},
  publisher={Taylor \& Francis}
}

@article{tenenbaum2011grow,
    title={How to grow a mind: Statistics, structure, and abstraction},
    author={Tenenbaum, Joshua B and Kemp, Charles and Griffiths, Thomas L
    and Goodman, Noah D},
    journal={science},
    volume={331},
    number={6022},
    pages={1279--1285},
    year={2011},
    publisher={American Association for the Advancement of Science}
}

@article{longuet1982perception,
author = {H Christopher Longuet-Higgins and Christopher S Lee},
title ={The Perception of Musical Rhythms},
journal = {Perception},
volume = {11},
number = {2},
pages = {115-128},
year = {1982},
doi = {10.1068/p110115},
note ={PMID: 7155765},
URL = { 
        https://doi.org/10.1068/p110115
},
eprint = { 
        https://doi.org/10.1068/p110115
}
,
    abstract = { There are many musical sequences in which the rhythm is evident from the mere durations of the notes. A simple theory is proposed of how a listener may infer the rhythm of such a sequence by comparing the note lengths. It is assumed that the listener forms an idea of the rhythm as the sequence unfolds, constructing and eliminating metrical hypotheses in the light of what he hears. The theory, which differs in some important respects from earlier proposals, has been implemented as a computer program. The program has been tested on a wide variety of musical examples, and its successes and failures are discussed in detail. }
}

@article{desain1998computational,
  title={Computational modeling of music cognition: problem or solution?},
  author={Desain, Peter and Honing, Henkjan and Vanthienen, Huub and Windsor, Luke},
  journal={Music Perception: An Interdisciplinary Journal},
  volume={16},
  number={1},
  pages={151--166},
  year={1998},
  publisher={University of California Press Journals}
}

@article{jones2002temporal,
  title={Temporal aspects of stimulus-driven attending in dynamic arrays},
  author={Jones, Mari Riess and Moynihan, Heather and MacKenzie, Noah and Puente, Jennifer},
  journal={Psychological science},
  volume={13},
  number={4},
  pages={313--319},
  year={2002},
  publisher={SAGE Publications}
}

@article{jones1976time,
    title={Time, our lost dimension: toward a new theory of perception,
        attention, and memory.},
    author={Jones, Mari R},
    journal={Psychological review}, 
    volume={83}, 
    number={5}, 
    pages={323}, 
    year={1976},
    publisher={American Psychological Association}
}

@inproceedings{whiteley2006bayesian,
    title={Bayesian Modelling of Temporal Structure in Musical Audio.},
    author={Whiteley, Nick and Cemgil, Ali Taylan and Godsill, Simon J},
    booktitle={ISMIR},
    pages={29--34},
    year={2006},
    organization={Citeseer}
}

@article{cemgil2000tempo,
    title={On tempo tracking: Tempogram representation and Kalman filtering},
    author={Cemgil, Ali Taylan and Kappen, Bert and Desain, Peter and
        Honing, Henkjan},
    journal={Journal of New Music Research},
    volume={29},
    number={4},
    pages={259--273},
    year={2000},
    publisher={Taylor \& Francis}
}

@inproceedings{nevill1997sequitur,
    title={Linear-time, incremental hierarchy inference for compression},
    author={Nevill-Manning, Craig G and Witten, Ian H},
    booktitle={Data Compression Conference, 1997. DCC'97. Proceedings},
    pages={3--11},
    year={1997},
    organization={IEEE}
}

@article{dehaene2015neural,
    title={The neural representation of sequences: from transition
        probabilities to algebraic patterns and linguistic trees},
    author={Dehaene, Stanislas and Meyniel, Florent and Wacongne, Catherine and
        Wang, Liping and Pallier, Christophe},
    journal={Neuron},
    volume={88},
    number={1},
    pages={2--19},
    year={2015}, 
    publisher={Elsevier}
}

@inproceedings{sidorov2014music,
    title={Music Analysis as a Smallest Grammar Problem.},
    author={Sidorov, Kirill A and Jones, Andrew and Marshall, A David},
    booktitle={ISMIR},
    pages={301--306},
    year={2014}
}

@article{large2015neural,
AUTHOR={Large, Edward W. and Herrera, Jorge A. and Velasco, Marc J.},   
TITLE={Neural Networks for Beat Perception in Musical Rhythm},      
JOURNAL={Frontiers in Systems Neuroscience},      
VOLUME={9},      
PAGES={159},     
YEAR={2015},      
URL={https://www.frontiersin.org/article/10.3389/fnsys.2015.00159},       
DOI={10.3389/fnsys.2015.00159},      
ISSN={1662-5137},   
ABSTRACT={Entrainment of cortical rhythms to acoustic rhythms has been hypothesized to be the neural correlate of pulse and meter perception in music. Dynamic attending theory first proposed synchronization of endogenous perceptual rhythms nearly 40 years ago, but only recently has the pivotal role of neural synchrony been demonstrated. Significant progress has since been made in understanding the role of neural oscillations and the neural structures that support synchronized responses to musical rhythm. Synchronized neural activity has been observed in auditory and motor networks, and has been linked with attentional allocation and movement coordination. Here we describe a neurodynamic model that shows how self-organization of oscillations in interacting sensory and motor networks could be responsible for the formation of the pulse percept in complex rhythms. In a pulse synchronization study, we test the model's key prediction that pulse can be perceived at a frequency for which no spectral energy is present in the amplitude envelope of the acoustic rhythm. The result shows that participants perceive the pulse at the theoretically predicted frequency. This model is one of the few consistent with neurophysiological evidence on the role of neural oscillation, and it explains a phenomenon that other computational models fail to explain. Because it is based on a canonical model, the predictions hold for an entire family of dynamical systems, not only a specific one. Thus, this model provides a theoretical link between oscillatory neurodynamics and the induction of pulse and meter in musical rhythm.}
}

@article{benetos2013automatic,
    title={Automatic music transcription: challenges and future directions},
    author={Benetos, Emmanouil and Dixon, Simon and Giannoulis, Dimitrios
    and Kirchhoff, Holger and Klapuri, Anssi},
    journal={Journal of Intelligent Information Systems},
    volume={41},
    number={3},
    pages={407--434},
    year={2013},
    publisher={Springer}
}

@inproceedings{salakhutdinov2012one,
    title={One-shot learning with a hierarchical nonparametric Bayesian
    model},
    author={Salakhutdinov, Ruslan and Tenenbaum, Joshua and Torralba,
    Antonio},
    booktitle={Proceedings of ICML Workshop on Unsupervised and Transfer
    Learning},
    pages={195--206},
    year={2012}
}

@article{laird1986chunking,
    title={Chunking in Soar: The anatomy of a general learning mechanism},
    author={Laird, John E and Rosenbloom, Paul S and Newell, Allen},
    journal={Machine learning},
    volume={1},
    number={1},
    pages={11--46},
    year={1986},
    publisher={Springer}
}

@inbook{huron2010musical,
  title={Musical expectancy and thrills},
  author={Huron, David and Margulis, Elizabeth Hellmuth},
  booktitle={Handbook of Music and Emotion},
  year={2010},
  publisher={Oxford University Press},
  chapter={21},
  pages={575--604}
}

@book{meyer1956emotion,
  title={Emotion and meaning in music},
  author={Meyer, Leonard B},
  publisher={Chicago University Press},
  year={1956}
}

@inproceedings{bock2014multi,
    title={A Multi-model Approach to Beat Tracking Considering Heterogeneous
           Music Styles.},
    author={B{\"o}ck, Sebastian and Krebs, Florian and Widmer, Gerhard}
}

@article{honing2009beat,
  title={Is Beat Induction Innate or Learned?},
  author={Honing, Henkjan and Ladinig, Olivia and H{\'a}den, G{\'a}bor P and Winkler, Istv{\'a}n},
  journal={Annals of the New York Academy of Sciences},
  volume={1169},
  number={1},
  pages={93--96},
  year={2009},
  publisher={Wiley Online Library}
}

@article{longuet1976perception,
author={Longuet-Higgins, H. C.},
title={Perception of melodies},
journal={Nature},
year={1976},
month={Oct},
day={01},
volume={263},
number={5579},
pages={646-653},
abstract={A computer program has been written which will transcribe a live performance of a classical melody into the equivalent of standard musical notation. It is intended to embody, in computational form, a psychological theory of how Western musicians perceive the rhythmic and tonal relationships between the notes of such melodies.},
issn={1476-4687},
doi={10.1038/263646a0},
url={https://doi.org/10.1038/263646a0}
}



@inproceedings{bock2016joint,
  title={Joint Beat and Downbeat Tracking with Recurrent Neural Networks.},
  author={Böck, Sebastian and Krebs, Florian and Widmer, Gerhard},
  booktitle={ISMIR},
  pages={255--261},
  year={2016}
}

@article{repp1990patterns,
  title={Patterns of expressive timing in performances of a Beethoven minuet by nineteen famous pianists},
  author={Repp, Bruno H},
  journal={The Journal of the Acoustical Society of America},
  volume={88},
  number={2},
  pages={622--641},
  year={1990},
  publisher={ASA}
}

@article{fitch2007perception,
    author = {Fitch, W. Tecumseh and Rosenfeld, Andrew J.},
    title = "{Perception and Production of Syncopated Rhythms}",
    journal = {Music Perception},
    volume = {25},
    number = {1},
    pages = {43-58},
    year = {2007},
    month = {09},
    abstract = "{THE PROCESSING OF COMPLEX, METRICALLY ambiguous rhythmic patterns, of the sort found in much popular music, remains poorly understood. We investigated listeners' abilities to perceive, process and produce complex, syncopated rhythmic patterns. Rhythmic complexity was varied along a continuum, quantified using an objective metric of syncopation suggested by Longuet-Higgins and Lee. Participants (a) tapped in time to the rhythms, (b) reproduced the same patterns given a steady pulse, and (c) recognized these patterns when replayed both immediately and after a 24-hour delay. Participants tended to reset the phase of their internally generated pulse with highly syncopated rhythms, reinterpreting or "re-hearing" the rhythm as less syncopated. High complexity in rhythmic stimuli can thus force a reorganization of their cognitive representation. Less complex rhythms were more robustly encoded than more complex syncopated rhythms in the delayed memory task. Syncopated rhythms provide a useful tool for future explorations of human rhythmic competence.}",
    issn = {0730-7829},
    doi = {10.1525/mp.2007.25.1.43},
    url = {https://doi.org/10.1525/mp.2007.25.1.43},
    eprint = {https://online.ucpress.edu/mp/article-pdf/25/1/43/301591/mp\_2007\_25\_1\_43.pdf},
}

@inproceedings{mckinney2004deviations,
  author       = {McKinney, M and Moelants, Dirk},
  booktitle    = {Conference on Interdisciplinary Musicology},
  editor       = {Parncutt, R and Kessler, A and Zimmer, F},
  isbn         = {3-200-00113-5},
  language     = {eng},
  pages        = {124--125},
  publisher    = {Department of Musicology, University of Graz},
  title        = {Deviations from the resonance theory of tempo induction},
  year         = {2004},
}


@article{davies2009evaluation,
  title={Evaluation methods for musical audio beat tracking algorithms},
  author={Davies, Matthew EP and Degara, Norberto and Plumbley, Mark D},
  journal={Queen Mary University of London, Centre for Digital Music, Tech. Rep. C4DM-TR-09-06},
  year={2009}
}

@book{juslin2011handbook,
  title={Handbook of music and emotion: Theory, research, applications},
  author={Juslin, Patrik N and Sloboda, John},
  year={2011},
  publisher={Oxford University Press}
}

@article{holzapfel2012selective,
  title={Selective sampling for beat tracking evaluation},
  author={Holzapfel, Andre and Davies, Matthew EP and Zapata, Jos{\'e} R and Oliveira, Jo{\~a}o Lobato and Gouyon, Fabien},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  volume={20},
  number={9},
  pages={2539--2548},
  year={2012},
  publisher={IEEE}
}

@article{patel2005influence,
  title={The influence of metricality and modality on synchronization with a beat},
  author={Patel, Aniruddh D and Iversen, John R and Chen, Yanqing and Repp, Bruno H},
  journal={Experimental brain research},
  volume={163},
  number={2},
  pages={226--238},
  year={2005},
  publisher={Springer}
}

@book{roberts1999latin,
  title={The Latin Tinge: The Impact of Latin American Music on the United States},
  author={Roberts, J.S.},
  isbn={9780190283841},
  url={https://books.google.com.ar/books?id=6mJMCAAAQBAJ},
  year={1999},
  publisher={Oxford University Press}
}

@article{neill2006rhythm-edm,
author = { Ben Neill },
title = {Pleasure Beats: Rhythm and the Aesthetics of Current Electronic Music},
journal = {Leonardo Music Journal},
volume = {-},
number = {},
pages = {3-6},
year = {2002},
doi = {10.1162/096112102762295052},
URL = {https://doi.org/10.1162/096112102762295052},
eprint = {https://doi.org/10.1162/096112102762295052},
abstract = {The division between high-art electronic music and pop electronic
    music is best defined in terms of rhythmic content. Pop electronic music
        uses repetitive beats, primarily in 4/4 time, but a new generation of
        composers is working within that structure to create what is
        essentially the new art music. This phenomenon is an outgrowth of such
        historical currents as minimalism and postmodernism, along with the
        continuing development of a global technoculture; it is part of a
        larger cultural shift in which art is becoming more connected with
        society rather than being created by and for specialists. This positive
        development is being accelerated by the rapid evolution of new
        technologies for producing and reproducing music today, as well as by
        new possibilities for distribution and dissemination of music
        electronically.}
}

@article{nobre2017anticipated,
  title={Anticipated moments: temporal structure in attention.},
  author={Nobre, AC and van Ede, F},
  journal={Nature reviews. Neuroscience},
  year={2017}
}

@article{kello2017hierarchical,
  title={Hierarchical temporal structure in music, speech and animal vocalizations: jazz is like a conversation, humpbacks sing like hermit thrushes},
  author={Kello, Christopher T and Dalla Bella, Simone and M{\'e}d{\'e}, Butovens and Balasubramaniam, Ramesh},
  journal={Journal of The Royal Society Interface},
  volume={14},
  number={135},
  pages={20170231},
  year={2017},
  publisher={The Royal Society}
}

@article{fitch2016dance,
  title={Dance, music, meter and groove: a forgotten partnership},
  author={Fitch, W},
  journal={Frontiers in human neuroscience},
  volume={10},
  pages={64},
  year={2016},
  publisher={Frontiers}
}

@article{griffiths2006optimal,
  title={Optimal predictions in everyday cognition},
  author={Griffiths, Thomas L and Tenenbaum, Joshua B},
  journal={Psychological science},
  volume={17},
  number={9},
  pages={767--773},
  year={2006},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@ARTICLE{tano2018learning,
   author = {{Tano}, P. and {Romano}, S. and {Sigman}, M. and {Salles}, A. and 
	{Figueira}, S.},
    title = "{Learning is Compiling: Experience Shapes Concept Learning by Combining Primitives in a Language of Thought}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1805.06924},
 primaryClass = "cs.AI",
 keywords = {Computer Science - Artificial Intelligence},
     year = 2018,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180506924T},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{clark2013whatever, 
	title={Whatever next? Predictive brains, situated agents, and the future of cognitive science}, 
	volume={36},
	DOI={10.1017/S0140525X12000477}, 
	number={3}, 
	journal={Behavioral and Brain Sciences}, 
	publisher={Cambridge University Press}, 
	author={Clark, Andy},
	year={2013}, 
	pages={181–204}
}

@article{friston2010free,
  title={The free-energy principle: a unified brain theory?},
  author={Friston, Karl},
  journal={Nature Reviews Neuroscience},
  volume={11},
  number={2},
  pages={127},
  year={2010},
  month={Feb},
  pages={127-138},
  publisher={Nature Publishing Group},
  issn={1471-0048},
  doi={10.1038/nrn2787},
  url={https://doi.org/10.1038/nrn2787}
}

@article{trost2017rhythmic,
  title={Rhythmic entrainment as a musical affect induction mechanism},
  author={Trost, WJ and Labb{\'e}, C and Grandjean, D},
  journal={Neuropsychologia},
  volume={96},
  pages={96--110},
  year={2017},
  publisher={Elsevier}
}


@article{lerdahl1992cognitive,
  title={Cognitive constraints on compositional systems},
  author={Lerdahl, Fred},
  journal={Contemporary Music Review},
  volume={6},
  number={2},
  pages={97--121},
  year={1992},
  publisher={Taylor \& Francis}
}

@article{margulis2008musical,
  title={Musical style, psychoaesthetics, and prospects for entropy as an analytic tool},
  author={Margulis, Elizabeth Hellmuth and Beatty, Andrew P},
  journal={Computer Music Journal},
  volume={32},
  number={4},
  pages={64--78},
  year={2008},
  publisher={MIT Press}
}

@article{witek2014syncopation,
  title={Syncopation, body-movement and pleasure in groove music},
  author={Witek, Maria AG and Clarke, Eric F and Wallentin, Mikkel and Kringelbach, Morten L and Vuust, Peter},
  journal={PloS one},
  volume={9},
  number={4},
  pages={e94446},
  year={2014},
  publisher={Public Library of Science}
}

@article{matthews2019sensation,
  title={The sensation of groove is affected by the interaction of rhythmic and harmonic complexity},
  author={Matthews, Tomas E and Witek, Maria AG and Heggli, Ole A and Penhune, Virginia B and Vuust, Peter},
  journal={PloS one},
  volume={14},
  number={1},
  pages={e0204539},
  year={2019},
  publisher={Public Library of Science}
}

@article{sloboda1991music,
  title={Music structure and emotional response: Some empirical findings},
  author={Sloboda, John A},
  journal={Psychology of music},
  volume={19},
  number={2},
  pages={110--120},
  year={1991},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{robertson2013synchronizing,
  title={Synchronizing sequencing software to a live drummer},
  author={Robertson, Andrew and Plumbley, Mark D},
  journal={Computer Music Journal},
  volume={37},
  number={2},
  pages={46--60},
  year={2013},
  publisher={MIT Press}
}

@article{stowell2009evaluation,
  title={Evaluation of live human--computer music-making: Quantitative and qualitative approaches},
  author={Stowell, Dan and Robertson, Andrew and Bryan-Kinns, Nick and Plumbley, Mark D},
  journal={International journal of human-computer studies},
  volume={67},
  number={11},
  pages={960--975},
  year={2009},
  publisher={Elsevier}
}

@article{largegrfnn,
    title={GrFNN Toolbox},
    author={Large, Edward and Kim, Ji Chul and Kim, Jung Nyo and Lerud,
            Karl and Flaig, Nicole Kristine and Farokhniaee, AmirAli and
            Wasserman, Charles S},
    url={http://musicdynamicslab.uconn.edu/home/multimedia/grfnn-toolbox/} 
}

@article{dixon2007evaluation,
  title={Evaluation of the audio beat tracking system beatroot},
  author={Dixon, Simon},
  journal={Journal of New Music Research},
  volume={36},
  number={1},
  pages={39--50},
  year={2007},
  publisher={Taylor \& Francis}
}

@inproceedings{zatorre2018we,
  title={Why Do We Love Music?},
  author={Zatorre, Robert J},
  booktitle={Cerebrum: the Dana forum on brain science},
  volume={2018},
  year={2018},
  organization={Dana Foundation}
}

@inproceedings{lartillot2008multi,
  title={Multi-Feature Modeling of Pulse Clarity: Design, Validation and Optimization.},
  author={Lartillot, Olivier and Eerola, Tuomas and Toiviainen, Petri and Fornari, Jose},
  booktitle={ISMIR},
  pages={521--526},
  year={2008},
  organization={Citeseer}
}


@inproceedings{thul2008rhythm,
  title={Rhythm Complexity Measures: A Comparison of Mathematical Models of Human Perception and Performance.},
  author={Thul, Eric and Toussaint, Godfried T},
  booktitle={ISMIR},
  pages={663--668},
  year={2008}
}

@article{thaut2015neurobiological,
  title={Neurobiological foundations of neurologic music therapy: rhythmic entrainment and the motor system},
  author={Thaut, Michael H and McIntosh, Gerald C and Hoemberg, Volker},
  journal={Frontiers in psychology},
  volume={5},
  pages={1185},
  year={2015},
  publisher={Frontiers}
}

@book{sacks2010musicophilia,
  title={Musicophilia: Tales of music and the brain},
  author={Sacks, Oliver},
  year={2010},
  publisher={Vintage Canada}
}

@inproceedings{raffel2014mir_eval,
  title={mir\_eval: A transparent implementation of common MIR metrics},
  author={Raffel, Colin and McFee, Brian and Humphrey, Eric J and Salamon, Justin and Nieto, Oriol and Liang, Dawen and Ellis, Daniel PW and Raffel, C Colin},
  booktitle={In Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR},
  year={2014},
  organization={Citeseer}
}

@article{elliott2014moving,
  title={Moving in time: Bayesian causal inference explains movement coordination to auditory beats},
  author={Elliott, Mark T and Wing, Alan M and Welchman, Andrew E},
  journal={Proceedings of the Royal Society B: Biological Sciences},
  volume={281},
  number={1786},
  pages={20140751},
  year={2014},
  publisher={The Royal Society}
}

@article{koelsch2013processing,
  title={Processing of hierarchical syntactic structure in music},
  author={Koelsch, Stefan and Rohrmeier, Martin and Torrecuso, Renzo and Jentschke, Sebastian},
  journal={Proceedings of the National Academy of Sciences},
  volume={110},
  number={38},
  pages={15443--15448},
  year={2013},
  publisher={National Acad Sciences}
}

@article{miguel2019tht,
  title={A continuous model of pulse clarity: towards inspecting affect through
      expectations in time},
  author={Miguel, Martin and Sigman, Mariano and Slezak, Diego Fernandez},
  journal={Poster at 2019 Meeting of the Society for Music Perception and
      Cognition},
  doi={10.17605/OSF.IO/FGVB2},
  year={2019},
}

@misc{miguel2019tapping,
title={Tapping to your own beat: experimental setup for exploring subjective tacti distribution and pulse clarity},
url={osf.io/7sqaw},
DOI={10.17605/OSF.IO/7SQAW},
publisher={OSF},
author={Miguel, Martin and Sigman, Mariano and Slezak, Diego Fernandez},
journal={Poster at 2019 Meeting of the Society for Music Perception and
  Cognition},
year={2019},
month={Oct}
}

@Article{zhang2017parkinsons,
author="Zhang, Shuai
and Liu, Dong
and Ye, Dan
and Li, Haiyu
and Chen, Feng",
title="Can music-based movement therapy improve motor dysfunction in patients with Parkinson's disease? Systematic review and meta-analysis",
journal="Neurological Sciences",
year="2017",
month="Sep",
day="01",
volume="38",
number="9",
pages="1629--1636",
abstract="This study aimed to quantify whether there is association between music-based movement therapy and motor dysfunction in patients with Parkinson's disease, and, if so, whether music-based movement therapy can be used as first-line non-pharmacological treatment. To conduct a systematic review and meta-analysis of clinical trials that examined the effect of music-based movement therapy on patient-relevant and disease-specific outcomes. Comprehensive literature was searched of PubMed, EMbase, and the Cochrane Library from inception to November 2016. Randomized controlled trial of patients with Parkinson's disease was searched to identify trials comparing music-based movement therapy with no music care. A total of 8 studies (11 analyses, 241 subjects) were included; all of them had acceptable quality by PEDro scale score. Studies based on any type of Parkinson's disease patients were combined and subgroup analyzed. Compared with the control group, the SMD of Berg Balance Scale score was 0.85(0.46 to 1.25), −0.60 (−0.98 to −0.22) in Parkinson Disease Questionnaire-39 summary index, −0.90s (−1.56 to −0.23) in Time Up and Go text, and −0.43 (−1.11 to 0.25) in Unified Parkinson's Disease Rating Scale Motor Subscale 3 as instrument methods for motor function. Secondary outcomes included cognitive function and quality of life. There was positive evidence to support the use of music-based movement therapy on treatment of motor function; there was neutral evidence to support the use of music for the treatment of cognitive function quality of life.",
issn="1590-3478",
doi="10.1007/s10072-017-3020-8",
url="https://doi.org/10.1007/s10072-017-3020-8"
}

@article{chen2008listening,
  title={Listening to musical rhythms recruits motor regions of the brain},
  author={Chen, Joyce L and Penhune, Virginia B and Zatorre, Robert J},
  journal={Cerebral cortex},
  volume={18},
  number={12},
  pages={2844--2854},
  year={2008},
  publisher={Oxford University Press}
}

@article{salimpoor2013interactions,
  title={Interactions between the nucleus accumbens and auditory cortices predict music reward value},
  author={Salimpoor, Valorie N and van den Bosch, Iris and Kovacevic, Natasa and McIntosh, Anthony Randal and Dagher, Alain and Zatorre, Robert J},
  journal={Science},
  volume={340},
  number={6129},
  pages={216--219},
  year={2013},
  publisher={American Association for the Advancement of Science}
}

@article{peretz2015speech,
    author = {Peretz, Isabelle and Vuvan, Dominique and Lagrois, Marie-Élaine and Armony, Jorge},
    year = {2015},
    month = {03},
    pages = {},
    title = {Neural overlap in processing music and speech},
    volume = {370},
    journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
    doi = {10.1098/rstb.2014.0090}
}

@article{chen2008moving,
  title={Moving on time: brain network for auditory-motor synchronization is modulated by rhythm complexity and musical training},
  author={Chen, Joyce L and Penhune, Virginia B and Zatorre, Robert J},
  journal={Journal of cognitive neuroscience},
  volume={20},
  number={2},
  pages={226--239},
  year={2008},
  publisher={MIT Press}
}

@article{peretz2015neural,
  title={Neural overlap in processing music and speech},
  author={Peretz, Isabelle and Vuvan, Dominique and Lagrois, Marie-{\'E}laine and Armony, Jorge L},
  journal={Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume={370},
  number={1664},
  pages={20140090},
  year={2015},
  publisher={The Royal Society}
}

@article{dube2003content,
  title={The content and structure of laypeople's concept of pleasure},
  author={Dub{\'e}, Laurette and Le Bel, Jordan},
  journal={Cognition and Emotion},
  volume={17},
  number={2},
  pages={263--295},
  year={2003},
  publisher={Taylor \& Francis}
}
@article{krause2010perception,
  title={Perception in action: the impact of sensory information on sensorimotor synchronization in musicians and non-musicians},
  author={Krause, Vanessa and Pollok, Bettina and Schnitzler, Alfons},
  journal={Acta psychologica},
  volume={133},
  number={1},
  pages={28--37},
  year={2010},
  publisher={Elsevier}
}


@article{mcauley2006time,
  title={The time of our lives: life span development of timing and event tracking.},
  author={McAuley, J Devin and Jones, Mari Riess and Holub, Shayla and Johnston, Heather M and Miller, Nathaniel S},
  journal={Journal of Experimental Psychology: General},
  volume={135},
  number={3},
  pages={348},
  year={2006},
  publisher={American Psychological Association}
}

@article{repp2005production,
  title={Production and synchronization of uneven rhythms at fast tempi},
  author={Repp, Bruno H and London, Justin and Keller, Peter E},
  journal={Music Perception: An Interdisciplinary Journal},
  volume={23},
  number={1},
  pages={61--78},
  year={2005},
  publisher={University of California Press Journals}
}

@article{barry2014sequential,
  title={Sequential processing in the equiprobable auditory Go/NoGo task: children vs. adults},
  author={Barry, Robert J and De Blasio, Frances M and Borchard, Jay P},
  journal={Clinical Neurophysiology},
  volume={125},
  number={10},
  pages={1995--2006},
  year={2014},
  publisher={Elsevier}
}

@article{daikoku2018motor,
  title={Motor reproduction of time interval depends on internal temporal cues in the brain: sensorimotor imagery in rhythm},
  author={Daikoku, Tatsuya and Takahashi, Yuji and Tarumoto, Nagayoshi and Yasuda, Hideki},
  journal={Frontiers in psychology},
  volume={9},
  pages={1873},
  year={2018},
  publisher={Frontiers}
}

@article{bock2017robod,
  title={Robod: a real-time online beat and offbeat drummer},
  author={Böck, S and Krebs, Florian and Durand, Amaury and Poll, S and Balsyte, Raminta},
  year={2017}
}

@misc{miguel2019tht_code,
    title="Distribution of THT code and Rhythmic Dataset",
    author="Martin A Miguel",
    url="https://osf.io/p3qtv/",
    year=2020,
    doi="10.17605/OSF.IO/P3QTV"
}

@misc{mirex2006beat_dataset,
    title="Mirex Beat Tracking training dataset.",
    url="https://www.music-ir.org/mirex/wiki/2019:Audio_Beat_Tracking",
    year=2006,
}

@article{snyder2001tapping,
  title={Tapping to ragtime: Cues to pulse finding},
  author={Snyder, Joel and Krumhansl, Carol L},
  journal={Music Perception: An Interdisciplinary Journal},
  volume={18},
  number={4},
  pages={455--489},
  year={2001},
  publisher={University of California Press Journals}
}

@article{schultz2019roles,
  title={The roles of musical expertise and sensory feedback in beat keeping and joint action},
  author={Schultz, Benjamin G and Palmer, Caroline},
  journal={Psychological research},
  volume={83},
  number={3},
  pages={419--431},
  year={2019},
  publisher={Springer}
}

@article{bavassi2017sensorimotor,
  title={Sensorimotor synchronization: neurophysiological markers of the asynchrony in a finger-tapping task},
  author={Bavassi, Luz and Kamienkowski, Juan E and Sigman, Mariano and Laje, Rodrigo},
  journal={Psychological research},
  volume={81},
  number={1},
  pages={143--156},
  year={2017},
  publisher={Springer}
}

@article{schultz2019schultz,
  title={The Schultz MIDI Benchmarking Toolbox for MIDI interfaces, percussion pads, and sound cards},
  author={Schultz, Benjamin G},
  journal={Behavior research methods},
  volume={51},
  number={1},
  pages={204--234},
  year={2019},
  publisher={Springer}
}

@article{schultz2016tap,
  title={Tap Arduino: An Arduino microcontroller for low-latency auditory feedback in sensorimotor synchronization experiments},
  author={Schultz, Benjamin G and van Vugt, Floris T},
  journal={Behavior research methods},
  volume={48},
  number={4},
  pages={1591--1607},
  year={2016},
  publisher={Springer}
}

@article{finney2016defense,
  title={In Defense of Linux, USB, and MIDI systems for sensorimotor experiments: A response to Schultz and van Vugt (2015)},
  author={Finney, Steven A},
  journal={Unpublished manuscript. Retrieved from http://www. sfinney. com/images/pdfs/sf/finney2016a. pdf},
  year={2016}
}

@article{shimizu2002measuring,
  title={Measuring keyboard response delays by comparing keyboard and joystick inputs},
  author={Shimizu, Hidemi},
  journal={Behavior Research Methods, Instruments, \& Computers},
  volume={34},
  number={2},
  pages={250--256},
  year={2002},
  publisher={Springer}
}

@article{segalowitz1990suitability,
  title={Suitability of the IBM XT, AT, and PS/2 keyboard, mouse, and game port as response devices in reaction time paradigms},
  author={Segalowitz, Sidney J and Graves, Roger E},
  journal={Behavior Research Methods, Instruments, \& Computers},
  volume={22},
  number={3},
  pages={283--289},
  year={1990},
  publisher={Springer}
}

@article{bridges2020timing,
  title={The timing mega-study: comparing a range of experiment generators, both lab-based and online},
  author={Bridges, David and Pitiot, Alain and MacAskill, Michael R and Peirce, Jonathan},
  year={2020},
  publisher={PsyArXiv}
}

@article{finney2001ftap,
  title={FTAP: A Linux-based program for tapping and music experiments},
  author={Finney, Steven A},
  journal={Behavior Research Methods, Instruments, \& Computers},
  volume={33},
  number={1},
  pages={65--72},
  year={2001},
  publisher={Springer}
}

@article{elliott2009mattap,
  title={MatTAP: A MATLAB toolbox for the control and analysis of movement synchronisation experiments},
  author={Elliott, Mark T and Welchman, Andrew E and Wing, Alan M},
  journal={Journal of Neuroscience Methods},
  volume={177},
  number={1},
  pages={250--257},
  year={2009},
  publisher={Elsevier}
}

@article{plant2004self,
  title={Self-validating presentation and response timing in cognitive paradigms: How and why?},
  author={Plant, Richard R and Hammond, Nick and Turner, Garry},
  journal={Behavior Research Methods, Instruments, \& Computers},
  volume={36},
  number={2},
  pages={291--303},
  year={2004},
  publisher={Springer}
}

@inproceedings{knorig2009fritzing,
  title={Fritzing: a tool for advancing electronic prototyping for designers},
  author={Kn{\"o}rig, Andr{\'e} and Wettach, Reto and Cohen, Jonathan},
  booktitle={Proceedings of the 3rd International Conference on Tangible and Embedded Interaction},
  pages={351--358},
  year={2009}
}

@misc{website:audacity,
    title = "Audacity (v2.2.1)",
    url = "www.audacityteam.org",
}

@book{raymond2003art,
  title={The art of Unix programming},
  author={Raymond, Eric S},
  year={2003},
  publisher={Addison-Wesley Professional}
}

@article{sanchez2018micromotion,
AUTHOR={Gonzalez-Sanchez, Victor E. and Zelechowska, Agata and Jensenius, Alexander Refsum},   
TITLE={Correspondences Between Music and Involuntary Human Micromotion During Standstill},      
JOURNAL={Frontiers in Psychology},      
VOLUME={9},      
PAGES={1382},     
YEAR={2018},      
URL={https://www.frontiersin.org/article/10.3389/fpsyg.2018.01382},       
DOI={10.3389/fpsyg.2018.01382},      
ISSN={1664-1078},   
ABSTRACT={The relationships between human body motion and music have been the focus of several studies characterizing the correspondence between voluntary motion and various sound features. The study of involuntary movement to music, however, is still scarce. Insight into crucial aspects of music cognition, as well as characterization of the vestibular and sensorimotor systems could be largely improved through a description of the underlying links between music and involuntary movement. This study presents an analysis aimed at quantifying involuntary body motion of a small magnitude (micromotion) during standstill, as well as assessing the correspondences between such micromotion and different sound features of the musical stimuli: pulse clarity, amplitude, and spectral centroid. A total of 71 participants were asked to stand as still as possible for 6 min while being presented with alternating silence and music stimuli: Electronic Dance Music (EDM), Classical Indian music, and Norwegian fiddle music (Telespringar). The motion of each participant's head was captured with a marker-based, infrared optical system. Differences in instantaneous position data were computed for each participant and the resulting time series were analyzed through cross-correlation to evaluate the delay between motion and musical features. The mean quantity of motion (QoM) was found to be highest across participants during the EDM condition. This musical genre is based on a clear pulse and rhythmic pattern, and it was also shown that pulse clarity was the metric that had the most significant effect in induced vertical motion across conditions. Correspondences were also found between motion and both brightness and loudness, providing some evidence of anticipation and reaction to the music. Overall, the proposed analysis techniques provide quantitative data and metrics on the correspondences between micromotion and music, with the EDM stimulus producing the clearest music-induced motion patterns. The analysis and results from this study are compatible with embodied music cognition and sensorimotor synchronization theories, and provide further evidence of the movement inducing effects of groove-related music features and human response to sound stimuli. Further work with larger data sets, and a wider range of stimuli, is necessary to produce conclusive findings on the subject.}
}

@incollection{lartillot2008matlab,
author="Lartillot, Olivier
and Toiviainen, Petri
and Eerola, Tuomas",
editor="Preisach, Christine
and Burkhardt, Hans
and Schmidt-Thieme, Lars
and Decker, Reinhold",
title="A Matlab Toolbox for Music Information Retrieval",
booktitle="Data Analysis, Machine Learning and Applications",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="261--268",
abstract="We present MIRToolbox, an integrated set of functions written in Matlab, dedicated to the extraction from audio files of musical features related, among others, to timbre, tonality, rhythm or form. The objective is to offer a state of the art of computational approaches in the area of Music Information Retrieval (MIR). The design is based on a modular framework: the different algorithms are decomposed into stages, formalized using a minimal set of elementary mechanisms, and integrating different variants proposed by alternative approaches --- including new strategies we have developed ---, that users can select and parametrize. These functions can adapt to a large area of objects as input.",
isbn="978-3-540-78246-9"
}


@article{fitch2013meter-vs-pulse,
AUTHOR={Fitch, W Tecumseh},   
TITLE={Rhythmic cognition in humans and animals: distinguishing meter and pulse perception},      
JOURNAL={Frontiers in Systems Neuroscience},      
VOLUME={7},      
PAGES={68},     
YEAR={2013},      
URL={https://www.frontiersin.org/article/10.3389/fnsys.2013.00068},       
DOI={10.3389/fnsys.2013.00068},      
ISSN={1662-5137},   
ABSTRACT={This paper outlines a cognitive and comparative perspective on human rhythmic cognition that emphasizes a key distinction between pulse perception and meter perception. Pulse perception involves the extraction of a regular pulse or “tactus” from a stream of events. Meter perception involves grouping of events into hierarchical trees with differing levels of “strength”, or perceptual prominence. I argue that metrically-structured rhythms are required to either perform or move appropriately to music (e.g., to dance). Rhythms, from this metrical perspective, constitute “trees in time.” Rhythmic syntax represents a neglected form of musical syntax, and warrants more thorough neuroscientific investigation. The recent literature on animal entrainment clearly demonstrates the capacity to extract the pulse from rhythmic music, and to entrain periodic movements to this pulse, in several parrot species and a California sea lion, and a more limited ability to do so in one chimpanzee. However, the ability of these or other species to infer hierarchical rhythmic trees remains, for the most part, unexplored (with some apparent negative results from macaques). The results from this animal comparative research, combined with new methods to explore rhythmic cognition neurally, provide exciting new routes for understanding not just rhythmic cognition, but hierarchical cognition more generally, from a biological and neural perspective.}
}

@inproceedings{gkiokas2017convolutional,
  title={Convolutional Neural Networks for Real-Time Beat Tracking: A Dancing Robot Application.},
  author={Gkiokas, Aggelos and Katsouros, Vassilis},
  booktitle={ISMIR},
  pages={286--293},
  year={2017}
}

@article{ellis2007beat,
  title={Beat tracking by dynamic programming},
  author={Ellis, Daniel PW},
  journal={Journal of New Music Research},
  volume={36},
  number={1},
  pages={51--60},
  year={2007},
  publisher={Taylor \& Francis}
}

@article{gkiokas2011ilsp,
  title={Ilsp Audio Tempo Estimation Algorithm For Mirex 2011},
  author={Gkiokas, Aggelos and Katsouros, Vassilis and Carayannis, George},
  journal={Proceedings of the Music Information Retrieval Evaluation eXchange (MIREX), Miami, USA},
  year={2011},
  publisher={Citeseer}
}

@inproceedings{khadkevich2012probabilistic,
  title={A probabilistic approach to simultaneous extraction of beats and downbeats},
  author={Khadkevich, Maksim and Fillon, Thomas and Richard, Ga{\"e}l and Omologo, Maurizio},
  booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={445--448},
  year={2012},
  organization={IEEE}
}

@article{oliveira2010ibt,
abstract = {This paper describes a tempo induction and beat tracking system based on the efficient strategy (initially introduced in the BeatRoot system [Dixon S., "Automatic extraction of tempo and beat from expressive performances." Journal of New Music Research, 30(1):39-58, 2001]) of competing agents processing musical input sequentially and considering parallel hypotheses regarding tempo and beats. In this paper, we propose to extend this strategy to the causal processing of continuous input data. The main reasons for this are threefold: providing more robustness to potentially noisy input data, permitting the parallel consideration of a number of low-level frame-based features as input, and opening the way to real-time uses of the system (as e.g. for a mobile robotic platform). The system is implemented in C++, permitting faster than real-time processing of audio data. It is integrated in the MARSYAS framework, and is therefore available under GPL for users and/or researchers. Detailed evaluation of the causal and non-causal versions of the system on common benchmark datasets show performances reaching those of state-of-the-art beat trackers. We propose a series of lines for future work based on careful analysis of the results. {\textcopyright} 2010 International Society for Music Information Retrieval.},
author = {Oliveira, Jo{\~{a}}o Lobato and Gouyon, Fabien and Martins, Luis Gustavo and Reis, Luis Paulo},
file = {:home/march/science/music/references/oliveira2010ibt-causal-tracking.pdf:pdf},
isbn = {9789039353813},
journal = {Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010},
pages = {291--296},
title = {{IBT: A real-time tempo and beat tracking system}},
year = {2010}
}


@article{kessler1984tonal,
  title={Tonal schemata in the perception of music in Bali and in the West},
  author={Kessler, Edward J and Hansen, Christa and Shepard, Roger N},
  journal={Music Perception},
  volume={2},
  number={2},
  pages={131--165},
  year={1984},
  publisher={University of California Press}
}

@article{repp2003rate,
  title={Rate limits in sensorimotor synchronization with auditory and visual sequences: The synchronization threshold and the benefits and costs of interval subdivision},
  author={Repp, Bruno H},
  journal={Journal of motor behavior},
  volume={35},
  number={4},
  pages={355--370},
  year={2003},
  publisher={Taylor \& Francis}
}

@article{martens2011tempo,
    author = {Martens, Peter A.},
    title = "{The Ambiguous Tactus: Tempo, Subdivision Benefit, And Three Listener Strategies}",
    journal = {Music Perception},
    volume = {28},
    number = {5},
    pages = {433-448},
    year = {2011},
    month = {06},
    abstract = "{the present study models listeners' tactus choices relative to the metric structure of fully musical excerpts using data from a tapping experiment. Viewed from the standpoint of metric structure, tactus was ambiguous between individuals and within excerpts, providing no evidence that this behavior has a global basis in tempo or in a subdivision benefit. Tactus was more consistent within individuals, however, when viewed as following from one of three basic strategies: (1) tapping with a subdivided pulse, (2) tapping with the fastest consistent pulse in the music (a pulse with no consistent subdivision), or (3) using a mixture of these two strategies based on inconsistent rhythmic activity at the musical surface. Music training correlated positively with the first of these strategies. Since individual listeners engage with musical meter in different ways, ambiguity of tactus should be an expected feature of any audience's response to metrical music.}",
    issn = {0730-7829},
    doi = {10.1525/mp.2011.28.5.433},
    url = {https://doi.org/10.1525/mp.2011.28.5.433},
    eprint = {https://online.ucpress.edu/mp/article-pdf/28/5/433/190687/mp\_2011\_28\_5\_433.pdf},
}

@inproceedings{burger2012movement,
abstract = {Listening to music makes us move in various ways. Several factors can affect the characteristics of these movements, including individual factors, musical features, or perceived emotional content of music. Music is based on regular and repetitive temporal patterns that give rise to a percept of pulse. From these basic metrical structures more complex temporal structures emerge, such as rhythm. It has been suggested that certain rhythmic features can induce movement in humans. Rhythmic structures vary in their degree of complexity and regularity, and one could expect that this variation influences movement patterns – for instance, when moving to rhythmically more complex music, the movements may also be more irregular. To investigating this relationship, sixty participants were presented with 30 musical stimuli representing different genres of popular music. All stimuli were 30 seconds long, non-vocal, and differed in their rhythmic complexity. Optical motion capture was used to record participants' movements. Two movement features were extracted from the data: Spatial Regularity and Temporal Regularity. Additionally, 12 beat-related musical features were extracted from the music stimuli. A subsequent correlational analysis revealed that beat-related musical features influenced the regularity of music-induced movement. In particular, a clear pulse and high percussiveness resulted in small spatial variation of participants' movements, whereas an unclear pulse and low percussiveness led to greater spatial variation of their movements. Additionally, temporal regularity was positively correlated to flux in the low frequencies (e.g., kick drum, bass guitar) and pulse clarity, suggesting that strong rhythmic components and a clear pulse encourage temporal regularity.},
author = {Burger, Birgitta and Thompson, Marc R. and Luck, Geoff and Saarikallio, Suvi and Toiviainen, Petri},
file = {:home/march/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burger et al. - 2012 - Music Moves Us Beat-Related Musical Features Influence Regularity of Music-Induced Movement.pdf:pdf},
isbn = {978-960-99845-1-5},
journal = {Proceedings of the 12th international Conference on Music Perception and Cognition},
number = {July},
pages = {183--187},
title = {{Music Moves Us: Beat-Related Musical Features Influence Regularity of Music-Induced Movement}},
url = {http://icmpc-escom2012.web.auth.gr/sites/default/files/papers/183_Proc.pdf},
year = {2012}
}


@article{trost2015brain,
    author = {Trost, Wiebke and Frühholz, Sascha and Cochrane, Tom and Cojan, Yann and Vuilleumier, Patrik},
    title = "{Temporal dynamics of musical emotions examined through intersubject synchrony of brain activity}",
    journal = {Social Cognitive and Affective Neuroscience},
    volume = {10},
    number = {12},
    pages = {1705-1721},
    year = {2015},
    month = {05},
    abstract = "{To study emotional reactions to music, it is important to consider the temporal dynamics of both affective responses and underlying brain activity. Here, we investigated emotions induced by music using functional magnetic resonance imaging (fMRI) with a data-driven approach based on intersubject correlations (ISC). This method allowed us to identify moments in the music that produced similar brain activity (i.e. synchrony) among listeners under relatively natural listening conditions. Continuous ratings of subjective pleasantness and arousal elicited by the music were also obtained for the music outside of the scanner. Our results reveal synchronous activations in left amygdala, left insula and right caudate nucleus that were associated with higher arousal, whereas positive valence ratings correlated with decreases in amygdala and caudate activity. Additional analyses showed that synchronous amygdala responses were driven by energy-related features in the music such as root mean square and dissonance, while synchrony in insula was additionally sensitive to acoustic event density. Intersubject synchrony also occurred in the left nucleus accumbens, a region critically implicated in reward processing. Our study demonstrates the feasibility and usefulness of an approach based on ISC to explore the temporal dynamics of music perception and emotion in naturalistic conditions.}",
    issn = {1749-5016},
    doi = {10.1093/scan/nsv060},
    url = {https://doi.org/10.1093/scan/nsv060},
    eprint = {https://academic.oup.com/scan/article-pdf/10/12/1705/27100403/nsv060.pdf},
}


@article{gold2019predictability,
  title={Predictability and uncertainty in the pleasure of music: a reward for learning?},
  author={Gold, Benjamin P and Pearce, Marcus T and Mas-Herrero, Ernest and Dagher, Alain and Zatorre, Robert J},
  journal={Journal of Neuroscience},
  volume={39},
  number={47},
  pages={9397--9409},
  year={2019},
  publisher={Soc Neuroscience}
}

@article{poudrier2018tapping,
  title={Tapping to Carter: Mensural determinacy in complex rhythmic sequences},
  author={Poudrier, {\`E}ve},
  journal={Empirical Musicology Review},
  volume={12},
  number={3-4},
  pages={277--315},
  year={2018}
}

@article{salimpoor2011anatomically,
  title={Anatomically distinct dopamine release during anticipation and experience of peak emotion to music},
  author={Salimpoor, Valorie N and Benovoy, Mitchel and Larcher, Kevin and Dagher, Alain and Zatorre, Robert J},
  journal={Nature neuroscience},
  volume={14},
  number={2},
  pages={257},
  year={2011},
  publisher={Nature Publishing Group}
}

@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, İlhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@article{miguel2020tht,
    author = {Miguel, Martin Alejandro AND Sigman, Mariano AND Fernandez Slezak, Diego},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {From beat tracking to beat expectation: Cognitive-based beat tracking for capturing pulse clarity through time},
    year = {2020},
    month = {11},
    volume = {15},
    url = {https://doi.org/10.1371/journal.pone.0242207},
    pages = {1-22},
    abstract = {Pulse is the base timing to which western music is commonly notated, generally expressed by a listener by performing periodic taps with their hand or foot. This cognitive construction helps organize the perception of timed events in music and is the most basic expectation in rhythms. The analysis of expectations, and more specifically the strength with which the beat is felt—the pulse clarity—has been used to analyze affect in music. Most computational models of pulse clarity, and rhythmic expectation in general, analyze the input as a whole, without exhibiting changes through a rhythmic passage. We present Tactus Hypothesis Tracker (THT), a model of pulse clarity over time intended for symbolic rhythmic stimuli. The model was developed based on ideas of beat tracking models that extract beat times from musical stimuli. Our model also produces possible beat interpretations for the rhythm, a fitness score for each interpretation and how these evolve in time. We evaluated the model’s pulse clarity by contrasting against tapping variability of human annotators achieving results comparable to a state-of-the-art pulse clarity model. We also analyzed the clarity metric dynamics on synthetic data that introduced changes in the beat, showing that our model presented doubt in the pulse estimation process and adapted accordingly to beat changes. Finally, we assessed if the beat tracking generated by the model was correct regarding listeners tapping data. We compared our beat tracking results with previous beat tracking models. The THT model beat tracking output showed generally correct estimations in phase but exhibits a bias towards a musically correct subdivision of the beat.},
    number = {11},
    doi = {10.1371/journal.pone.0242207}
}

@inproceedings{miguel20212d,
  author       = {Martin A Miguel and Diego Fernandez Slezak},
  title        = {{Modeling beat uncertainty as a 2D distribution of period and
                  phase: a MIR task proposal}},
  booktitle    = {{Proceedings of the 22nd International Society for Music
                  Information Retrieval Conference}},
  year         = 2021,
  pages        = {452-459},
  publisher    = {ISMIR},
  address      = {Online},
  month        = nov,
  venue        = {Online},
  doi          = {10.5281/zenodo.5624639},
  url          = {https://doi.org/10.5281/zenodo.5624639}
}

@inproceedings{pironio2021clarity,
    author       = {Nicolás Pironio and Diego Fernandez Slezak and
                    Martin A Miguel},
    title        = {{Pulse clarity metrics developed from a deep
                    learning beat tracking model}},
    booktitle    = {{Proceedings of the 22nd International Society for Music
                    Information Retrieval Conference}},
    year         = 2021,
    pages        = {525-530},
    publisher    = {ISMIR},
    address      = {Online},
    month        = nov,
    venue        = {Online},
    doi          = {10.5281/zenodo.5625692},
    url          = {https://doi.org/10.5281/zenodo.5625692}
}

@article{noulhiane2007emotional,
  title={How emotional auditory stimuli modulate time perception.},
  author={Noulhiane, Marion and Mella, Nathalie and Samson, S{\'e}verine and Ragot, Richard and Pouthas, Viviane},
  journal={Emotion},
  volume={7},
  number={4},
  pages={697},
  year={2007},
  publisher={American Psychological Association}
}

@article{falk2016expected,
author = {Simone Falk and Simone Dalla Bella},
title = {It is better when expected: aligning speech and motor rhythms enhances verbal processing},
journal = {Language, Cognition and Neuroscience},
volume = {31},
number = {5},
pages = {699-708},
year  = {2016},
publisher = {Routledge},
doi = {10.1080/23273798.2016.1144892},
URL = { 
        https://doi.org/10.1080/23273798.2016.1144892
},
eprint = { 
        https://doi.org/10.1080/23273798.2016.1144892
}
}

@article{mckinney2007evaluation,
author = { M. F.   McKinney  and  D.   Moelants  and  M. E. P.   Davies  and  A.   Klapuri },
title = {Evaluation of Audio Beat Tracking and Music Tempo Extraction Algorithms},
journal = {Journal of New Music Research},
volume = {36},
number = {1},
pages = {1-16},
year  = {2007},
publisher = {Routledge},
doi = {10.1080/09298210701653252},
URL = { 
        https://doi.org/10.1080/09298210701653252
},
eprint = { 
        https://doi.org/10.1080/09298210701653252
}
}

@article{babjack2015chronos,
author={Babjack, Destiny L.
and Cernicky, Brandon
and Sobotka, Andrew J.
and Basler, Lee
and Struthers, Devon
and Kisic, Richard
and Barone, Kimberly
and Zuccolotto, Anthony P.},
title={Reducing audio stimulus presentation latencies across studies, laboratories, and hardware and operating system configurations},
journal={Behavior Research Methods},
year={2015},
month={Sep},
day={01},
volume={47},
number={3},
pages={649-665},
abstract={Using differing computer platforms and audio output devices to deliver audio stimuli often introduces (1) substantial variability across labs and (2) variable time between the intended and actual sound delivery (the sound onset latency). Fast, accurate audio onset latencies are particularly important when audio stimuli need to be delivered precisely as part of studies that depend on accurate timing (e.g., electroencephalographic, event-related potential, or multimodal studies), or in multisite studies in which standardization and strict control over the computer platforms used is not feasible. This research describes the variability introduced by using differing configurations and introduces a novel approach to minimizing audio sound latency and variability. A stimulus presentation and latency assessment approach is presented using E-Prime and Chronos (a new multifunction, USB-based data presentation and collection device). The present approach reliably delivers audio stimuli with low latencies that vary by ≤1 ms, independent of hardware and Windows operating system (OS)/driver combinations. The Chronos audio subsystem adopts a buffering, aborting, querying, and remixing approach to the delivery of audio, to achieve a consistent 1-ms sound onset latency for single-sound delivery, and precise delivery of multiple sounds that achieves standard deviations of 1/10th of a millisecond without the use of advanced scripting. Chronos's sound onset latencies are small, reliable, and consistent across systems. Testing of standard audio delivery devices and configurations highlights the need for careful attention to consistency between labs, experiments, and multiple study sites in their hardware choices, OS selections, and adoption of audio delivery systems designed to sidestep the audio latency variability issue.},
issn={1554-3528},
doi={10.3758/s13428-015-0608-x},
url={https://doi.org/10.3758/s13428-015-0608-x}
}

@article{mathot2012open,
author={Math{\^o}t, Sebastiaan
and Schreij, Daniel
and Theeuwes, Jan},
title={OpenSesame: An open-source, graphical experiment builder for the social sciences},
journal={Behavior Research Methods},
year={2012},
month={Jun},
day={01},
volume={44},
number={2},
pages={314-324},
abstract={In the present article, we introduce OpenSesame, a graphical experiment builder for the social sciences. OpenSesame is free, open-source, and cross-platform. It features a comprehensive and intuitive graphical user interface and supports Python scripting for complex tasks. Additional functionality, such as support for eyetrackers, input devices, and video playback, is available through plug-ins. OpenSesame can be used in combination with existing software for creating experiments.},
issn={1554-3528},
doi={10.3758/s13428-011-0168-7},
url={https://doi.org/10.3758/s13428-011-0168-7}
}

@article{peirce2019psychopy,
author={Peirce, Jonathan
and Gray, Jeremy R.
and Simpson, Sol
and MacAskill, Michael
and H{\"o}chenberger, Richard
and Sogo, Hiroyuki
and Kastman, Erik
and Lindel{\o}v, Jonas Kristoffer},
title={PsychoPy2: Experiments in behavior made easy},
journal={Behavior Research Methods},
year={2019},
month={Feb},
day={01},
volume={51},
number={1},
pages={195-203},
abstract={PsychoPy is an application for the creation of experiments in behavioral science (psychology, neuroscience, linguistics, etc.) with precise spatial control and timing of stimuli. It now provides a choice of interface; users can write scripts in Python if they choose, while those who prefer to construct experiments graphically can use the new Builder interface. Here we describe the features that have been added over the last 10 years of its development. The most notable addition has been that Builder interface, allowing users to create studies with minimal or no programming, while also allowing the insertion of Python code for maximal flexibility. We also present some of the other new features, including further stimulus options, asynchronous time-stamped hardware polling, and better support for open science and reproducibility. Tens of thousands of users now launch PsychoPy every month, and more than 90 people have contributed to the code. We discuss the current state of the project, as well as plans for the future.},
issn={1554-3528},
doi={10.3758/s13428-018-01193-y},
url={https://doi.org/10.3758/s13428-018-01193-y}
}

@article{kleiner2007psychtoolbox,
  title={What's new in Psychtoolbox-3?},
  author={Kleiner, Mario and Brainard, David and Pelli, Denis},
  year={2007},
  publisher={Pion Ltd.}
}

@misc{dimigen2020eeg,
  title={Coregistration of eye movements and EEG in natural reading: Analyses and Review},
  url={osf.io/cd9am},
  publisher={OSF},
  author={Dimigen, Olaf},
  year={2020},
  month={Sep}
}

@article{bavassi2017eeg,
	author={Bavassi, Luz
	and Kamienkowski, Juan E.
	and Sigman, Mariano
	and Laje, Rodrigo},
	title={Sensorimotor synchronization: neurophysiological markers of the asynchrony in a finger-tapping task},
	journal={Psychological Research},
	year={2017},
	month={Jan},
	day={01},
	volume={81},
	number={1},
	pages={143-156},
	abstract={Sensorimotor synchronization (SMS) is a form of referential behavior in which an action is coordinated with a predictable external stimulus. The neural bases of the synchronization ability remain unknown, even in the simpler, paradigmatic task of finger tapping to a metronome. In this task the subject is instructed to tap in synchrony with a periodic sequence of brief tones, and the time difference between each response and the corresponding stimulus tone (asynchrony) is recorded. We make a step towards the identification of the neurophysiological markers of SMS by recording high-density EEG event-related potentials and the concurrent behavioral response-stimulus asynchronies during an isochronous paced finger-tapping task. Using principal component analysis, we found an asymmetry between the traces for advanced and delayed responses to the stimulus, in accordance with previous behavioral observations from perturbation studies. We also found that the amplitude of the second component encodes the higher-level percept of asynchrony 100 ms after the current stimulus. Furthermore, its amplitude predicts the asynchrony of the next step, past 300 ms from the previous stimulus, independently of the period length. Moreover, the neurophysiological processing of synchronization errors is performed within a fixed-duration interval after the stimulus. Our results suggest that the correction of a large asynchrony in a periodic task and the recovery of synchrony after a perturbation could be driven by similar neural processes.},
	issn={1430-2772},
	doi={10.1007/s00426-015-0721-6},
	url={https://doi.org/10.1007/s00426-015-0721-6}
}

@article{care2020search,
author={Care, Damian
and Bianchi, Bruno
and Kamienkowski, Juan Esteban
and Ison, Matias J.},
title={Face processing in free viewing visual search: An investigation using concurrent EEG and eye movement recordings},
journal={Journal of Vision},
year={2020},
month={Oct},
day={20},
volume={20},
number={11},
pages={1691-1691},
abstract={The neural underpinnings of face processing have largely been investigated under fixed-gaze. The electrophysiological hallmark of face processing is the N170 component, which emerges in the occipitotemporal cortex around 170 ms after stimulus onset and is characterized by a larger amplitude in response to pictures of faces, compared to other object categories. Several properties modulating this component have been extensively studied, including its sensitivity to face inversion and low-level manipulations (see, e.g. Rossion, 2015). However, little is known about face processing in natural viewing. A recent study (Kamienkowski, Varatharajah, Sigman {\&}amp; Ison, 2018) reported strong fixation-related potentials (fERPs) to faces in a free-viewing visual search task and a significant difference between target and distractor faces around 170 ms after fixation. In this study, we co-registered EEG and eye tracking to investigate fERPs to pictures of faces and objects during visual search and free exploration. Participants were asked to search for one target face or object stimulus during visual search blocks (VS) and to explore an array of faces and objects embedded in random noise during free exploration (EXP). We hypothesized that a larger N170 would be elicited by fixations to faces in comparison to objects. Importantly, based on a recently proposed framework (Kamienkowski et al., 2018), we also hypothesized that the early target detection that has previously been reported largely reflects saccade inhibition, and would therefore be activated differently for easy distractors (different categories) than hard distractors (same category). Preliminary analyses of the fERPs show robust early potentials, but no differential activation for different categories. Possible interpretations in terms of differences in low-level features, reduced signal to noise ratio and superposition of brain potentials are discussed. },
issn={1534-7362},
doi={10.1167/jov.20.11.1691},
url={https://doi.org/10.1167/jov.20.11.1691}
}

@article{fujii2011synchronization,
  title={Synchronization error of drum kit playing with a metronome at different tempi by professional drummers},
  author={Fujii, Shinya and Hirashima, Masaya and Kudo, Kazutoshi and Ohtsuki, Tatsuyuki and Nakamura, Yoshihiko and Oda, Shingo},
  journal={Music Perception: An Interdisciplinary Journal},
  volume={28},
  number={5},
  pages={491--503},
  year={2011},
  publisher={University of California Press USA}
}

@article{repp2007tapping,
  title={Tapping to a very slow beat: a comparison of musicians and nonmusicians},
  author={Repp, Bruno H and Doggett, Rebecca},
  journal={Music Perception},
  volume={24},
  number={4},
  pages={367--376},
  year={2007},
  publisher={University of California Press USA}
}

@ARTICLE{klapuri2006meter,  
    author={Klapuri, A.P. and Eronen, A.J. and Astola, J.T.},  
    journal={IEEE Transactions on Audio, Speech, and Language Processing},   
    title={Analysis of the meter of acoustic musical signals},   
    year={2006},  
    volume={14},  
    number={1},  
    pages={342-355},  
    doi={10.1109/TSA.2005.854090}}
} 

@misc{peyre2020computational,
      title={Computational Optimal Transport}, 
      author={Gabriel Peyré and Marco Cuturi},
      year={2020},
      eprint={1803.00567},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@book{fisher1993circular, 
      place={Cambridge}, 
      title={Statistical Analysis of Circular Data}, 
      DOI={10.1017/CBO9780511564345}, 
      publisher={Cambridge University Press}, 
      author={Fisher, N. I.}, 
      year={1993}
}

@article{luck2008emotion,
author = {Geoff Luck and Petri Toiviainen and Jaakko Erkkilä and Olivier Lartillot and Kari Riikkilä and Arto Mäkelä and Kimmo Pyhäluoto and Heikki Raine and Leila Varkila and Jukka Värri},
title ={Modelling the relationships between emotional responses to, and musical                 content of, music therapy improvisations},
journal = {Psychology of Music},
volume = {36},
number = {1},
pages = {25-45},
year = {2008},
doi = {10.1177/0305735607079714},

URL = { 
        https://doi.org/10.1177/0305735607079714
    
},
eprint = { 
        https://doi.org/10.1177/0305735607079714
    
}
,
    abstract = { This article reports a study in which listeners were asked to provide continuous ratings of perceived emotional content of clinical music therapy improvisations. Participants were presented with 20 short excerpts of music therapy improvisations, and had to rate perceived activity, pleasantness and strength using a computer-based slider interface. A total of nine musical features relating to various aspects of the music (timing, register, dynamics, tonality, pulse clarity and sensory dissonance) were extracted from the excerpts, and relationships between these features and participants' emotion ratings were investigated. The data were analysed in three stages. First, inter-dimension correlations revealed that ratings of activity and pleasantness were moderately negatively correlated, activity and strength were strongly positively correlated, and strength and pleasantness were moderately negatively correlated. Second, a series of cross-correlation analyses revealed that the temporal lag between musical features and listeners' dimension ratings differed across both variables and dimensions. Finally, a series of linear regression analyses produced significant feature prediction models for each of the three dimensions, accounting for 80 percent (activity), 57 percent (pleasantness ) and 84 percent (strength) of the variance in participants' ratings. Activity was best predicted by high note density and high pulse clarity, pleasantness by low note density and high tonal clarity, and strength by high mean velocity and low note density. The results are discussed in terms of their fit with other work reported in the music psychology literature, and their relevance to clinical music therapy research and practice. }
}


@article{vuust2018incongruity,
author = {Vuust, Peter and Dietz, Martin J. and Witek, Maria and Kringelbach, Morten L.},
title = {Now you hear it: a predictive coding model for understanding rhythmic incongruity},
journal = {Annals of the New York Academy of Sciences},
volume = {1423},
number = {1},
pages = {19-29},
keywords = {rhythm, music, predictive coding, brain},
doi = {https://doi.org/10.1111/nyas.13622},
url = {https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/nyas.13622},
eprint = {https://nyaspubs.onlinelibrary.wiley.com/doi/pdf/10.1111/nyas.13622},
abstract = {Abstract Rhythmic incongruity in the form of syncopation is a prominent feature of many contemporary musical styles. Syncopations afford incongruity between rhythmic patterns and the meter, giving rise to mental models of differently accented isochronous beats. Syncopations occur either in isolation or as part of rhythmic patterns, so-called grooves. On the basis of the predictive coding framework, we discuss how brain processing of rhythm can be seen as a special case of predictive coding. We present a simple, yet powerful model for how the brain processes rhythmic incongruity: the model for predictive coding of rhythmic incongruity. Our model proposes that a given rhythm's syncopation and its metrical uncertainty (precision) is at the heart of how the brain models rhythm and meter based on priors, predictions, and prediction error. Our minimal model can explain prominent features of brain processing of syncopation: why isolated syncopations lead to stronger prediction error in the brains of musicians, as evidenced by larger event-related potentials to rhythmic incongruity, and why we all experience a stronger urge to move to grooves with a medium level of syncopation compared with low and high levels of syncopation.},
year = {2018}
}

@inproceedings{moelants2002preferred,
  author       = {Moelants, Dirk},
  booktitle    = {Proceedings of the 7th International Conference on Music Perception and Cognition / C. Stevens, D. Burnham, G. McPherson, E. Schubert, J. Renwick (eds.). - Sydney, Adelaide, Causal Productions, 2002},
  language     = {eng},
  pages        = {580--583},
  title        = {Preferred tempo reconsidered.},
  year         = {2002},
}

@inproceedings{moelants2004tempo,
  title={Tempo perception and musical content: What makes a piece fast, slow or
         temporally ambiguous?},
  author={Moelants, Dirk and McKinney, Martin},
  booktitle={Proceedings of the 8th International Conference on Music Perception and Cognition},
  pages={558--562},
  year={2004}
}

@article{gouyon2006comparison,  
    author={Gouyon, F. and Klapuri, A. and Dixon, S. and Alonso, M. and Tzanetakis, G. and Uhle, C. and Cano, P.},  
    journal={IEEE Transactions on Audio, Speech, and Language Processing},   
    title={An experimental comparison of audio tempo induction algorithms},   
    year={2006},  
    volume={14},  
    number={5},  
    pages={1832-1844},  
    doi={10.1109/TSA.2005.858509}
}

@inproceedings{schreiber2020modeling,
  title={Modeling and estimating local tempo: A case study on Chopin’s Mazurkas},
  author={Schreiber, Hendrik and Zalkow, Frank and M{\"u}ller, Meinard},
  booktitle={Proceedings of the International Society for Music Information Retrieval Conference (ISMIR), Montreal, Quebec, Canada},
  year={2020}
}

@inproceedings{bock2015accurate,
  title={Accurate Tempo Estimation Based on Recurrent Neural Networks and Resonating Comb Filters.},
  author={B{\"o}ck, Sebastian and Krebs, Florian and Widmer, Gerhard},
  booktitle={ISMIR},
  pages={625--631},
  year={2015}
}

@inproceedings{krebs2015efficient,
title={An Efficient State-Space Model for Joint Tempo and Meter
Tracking.},
author={Krebs, Florian and B{\"o}ck, Sebastian and Widmer, Gerhard},
booktitle={ISMIR},
pages={72--78},
year={2015}
}

@misc{eerola2020dataset,
  title={Ground-truth data for selected perceptual features},
  url={osf.io/6wqh5},
  DOI={10.17605/OSF.IO/6WQH5},
  publisher={OSF},
  author={Eerola, Tuomas},
  year={2020},
  month={Jul}
}

@article{miguel2021tapping,
author={Miguel, Martin A. and Riera, Pablo and Fernández Slezak, Diego},
title={A simple and cheap setup for timing tapping responses
       synchronized to auditory stimuli},
    journal={Behavior Research Methods (in press)},
    year={2021},
    doi={10.3758/s13428-021-01653-y},
    url={https://doi.org/10.3758/s13428-021-01653-y}
}

@article{palmer1990mental,
  title={Mental representations for musical meter.},
  author={Palmer, Caroline and Krumhansl, Carol L},
  journal={Journal of Experimental Psychology: Human Perception and Performance},
  volume={16},
  number={4},
  pages={728},
  year={1990},
  publisher={American Psychological Association},
  doi={10.1037/0096-1523.16.4.728}
}

@phdthesis{pearce2005construction,
  title={The construction and evaluation of statistical models of melodic structure in music perception and composition},
  author={Pearce, Marcus Thomas},
  year={2005},
  school={City University London}
}

@article{wundt1948principles,
    title={Principles of physiological psychology, 1873.},
    author={Wundt, Wilhelm},
    year={1948},
    publisher={Appleton-Century-Crofts}
}

@book{berlyne1971aesthetics,
  title={Aesthetics and Psychobiology},
  author={Berlyne, D.E. and berlyne, de},
  isbn={9780390086709},
  lccn={70165204},
  series={Century psychology series},
  url={https://books.google.com.ar/books?id=o5TWAAAAMAAJ},
  year={1971},
  publisher={Appleton-Century-Crofts}
}

@article{berlyne1969arosual,
    author = {Berlyne, D. E.},
    title = {AROUSAL, REWARD AND LEARNING*},
    journal = {Annals of the New York Academy of Sciences},
    volume = {159},
    number = {3},
    pages = {1059-1070},
    doi = {https://doi.org/10.1111/j.1749-6632.1969.tb12997.x},
    url = {https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-6632.1969.tb12997.x},
    eprint = {https://nyaspubs.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1749-6632.1969.tb12997.x},
    year = {1969}
}

@article{chimel2017inverted,
    author = {Anthony Chmiel and Emery Schubert},
title ={Back to the inverted-U for music preference: A review of the literature},
journal = {Psychology of Music},
volume = {45},
number = {6},
pages = {886-909},
year = {2017},
doi = {10.1177/0305735617697507},

URL = { 
        https://doi.org/10.1177/0305735617697507
    
},
eprint = { 
        https://doi.org/10.1177/0305735617697507
    
}
,
    abstract = { This study investigated the inverted-U model of preference for music as a function of collative variables (especially familiarity and complexity) over the last 115 years. The results of 57 studies on music preference were categorized according to their patterns of preference. Fifty of the 57 studies (87.7\%) were categorized as compatible with an overarching (segmented) inverted-U model, while the results of five studies (8.8\%) were interpreted as mixed, showing both compatible and incompatible results. Two studies (3.5\%) were categorized as completely incompatible with the model. In contrast to authors who describe the model as defunct, this review has observed that studies producing results compatible with the inverted-U are still prevalent. We propose that while there may be inconsistencies with Berlyne’s psychobiological theory from a scientific, arousal-based standpoint, the inverted-U model is able to explain a considerable amount of data. Rather, it seems that research interests have moved elsewhere, but caution is urged in asserting denial or dismissal of the relationship in music preference research. }
}

@article{teki2011distinct,
	author = {Teki, Sundeep and Grube, Manon and Kumar, Sukhbinder and Griffiths, Timothy D.},
	title = {Distinct Neural Substrates of Duration-Based and Beat-Based Auditory Timing},
	volume = {31},
	number = {10},
	pages = {3805--3812},
	year = {2011},
	doi = {10.1523/JNEUROSCI.5561-10.2011},
	publisher = {Society for Neuroscience},
	abstract = {Research on interval timing strongly implicates the cerebellum and the basal ganglia as part of the timing network of the brain. Here we tested the hypothesis that the brain uses differential timing mechanisms and networks{\textemdash}specifically, that the cerebellum subserves the perception of the absolute duration of time intervals, whereas the basal ganglia mediate perception of time intervals relative to a regular beat. In a functional magnetic resonance imaging experiment, we asked human subjects to judge the difference in duration of two successive time intervals as a function of the preceding context of an irregular sequence of clicks (where the task relies on encoding the absolute duration of time intervals) or a regular sequence of clicks (where the regular beat provides an extra cue for relative timing). We found significant activations in an olivocerebellar network comprising the inferior olive, vermis, and deep cerebellar nuclei including the dentate nucleus during absolute, duration-based timing and a striato-thalamo-cortical network comprising the putamen, caudate nucleus, thalamus, supplementary motor area, premotor cortex, and dorsolateral prefrontal cortex during relative, beat-based timing. Our results support two distinct timing mechanisms and underlying subsystems: first, a network comprising the inferior olive and the cerebellum that acts as a precision clock to mediate absolute, duration-based timing, and second, a distinct network for relative, beat-based timing incorporating a striato-thalamo-cortical network.},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/31/10/3805},
	eprint = {https://www.jneurosci.org/content/31/10/3805.full.pdf},
	journal = {Journal of Neuroscience}
}

@article{winkler2009infants,
	author = {Winkler, Istv{\'a}n and H{\'a}den, G{\'a}bor P. and Ladinig, Olivia and Sziller, Istv{\'a}n and Honing, Henkjan},
	title = {Newborn infants detect the beat in music},
	volume = {106},
	number = {7},
	pages = {2468--2471},
	year = {2009},
	doi = {10.1073/pnas.0809035106},
	publisher = {National Academy of Sciences},
	abstract = {To shed light on how humans can learn to understand music, we need to discover what the perceptual capabilities with which infants are born. Beat induction, the detection of a regular pulse in an auditory signal, is considered a fundamental human trait that, arguably, played a decisive role in the origin of music. Theorists are divided on the issue whether this ability is innate or learned. We show that newborn infants develop expectation for the onset of rhythmic cycles (the downbeat), even when it is not marked by stress or other distinguishing spectral features. Omitting the downbeat elicits brain activity associated with violating sensory expectations. Thus, our results strongly support the view that beat perception is innate.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/106/7/2468},
	eprint = {https://www.pnas.org/content/106/7/2468.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{ladinig2009meter,
    author = {Ladinig, Olivia and Honing, Henkjan and Hááden, Gáábor and Winkler, Istváán},
    title = "{Probing Attentive and Preattentive Emergent Meter in Adult Listeners without Extensive Music Training}",
    journal = {Music Perception},
    volume = {26},
    number = {4},
    pages = {377-386},
    year = {2009},
    month = {04},
    abstract = "{BEAT AND METER INDUCTION ARE CONSIDERED important structuring mechanisms underlying the perception of rhythm. Meter comprises two or more levels of hierarchically ordered regular beats with different periodicities. When listening to music, adult listeners weight events within a measure in a hierarchical manner. We tested if listeners without advanced music training form such hierarchical representations for a rhythmical sound sequence under different attention conditions (Attend, Unattend, and Passive). Participants detected occasional weakly and strongly syncopated rhythmic patterns within the context of a strictly metrical rhythmical sound sequence. Detection performance was better and faster when syncopation occurred in a metrically strong as compared to a metrically weaker position. Compatible electrophysiological differences (earlier and higher-amplitude MMN responses) were obtained when participants did not attend the rhythmical sound sequences. These data indicate that hierarchical representations for rhythmical sound sequences are formed preattentively in the human auditory system.}",
    issn = {0730-7829},
    doi = {10.1525/mp.2009.26.4.377},
    url = {https://doi.org/10.1525/mp.2009.26.4.377},
    eprint = {https://online.ucpress.edu/mp/article-pdf/26/4/377/190428/mp\_2009\_26\_4\_377.pdf},
}

@article{ziv1978compression,  
	author={Ziv, J. and Lempel, A.},  
	journal={IEEE Transactions on Information Theory},   
	title={Compression of individual sequences via variable-rate coding},   
	year={1978},  
	volume={24},  
	number={5},  
	pages={530-536},  
	doi={10.1109/TIT.1978.1055934}
}

@book{london2012hearing,
  title={Hearing in time: Psychological aspects of musical meter},
  author={London, Justin},
  year={2012},
  publisher={Oxford University Press}
}

@article{essens1995complexity,
author={Essens, Peter},
title={Structuring temporal sequences: Comparison of models and factors of complexity},
journal={Perception {\&} Psychophysics},
year={1995},
month={Jun},
day={01},
volume={57},
number={4},
pages={519-532},
abstract={Two stages for structuring tone sequences have been distinguished by Povel and Essens (1985). In the first, a mental clock segments a sequence into equal time units (clock model); in the second, intervals are specified in terms of subdivisions of these units. The present findings support the clock model in that it predicts human performance better than three other algorithmic models. Two further experiments in which clock and subdivision characteristics were varied did not support the hypothesized effect of the nature of the subdivisions on complexity. A model focusing on the variations in the beat-anchored envelopes of the tone clusters was proposed. Errors in reproduction suggest a dual-code representation comprising temporaland figural characteristics. The temporal part of the representation is based on the clock model but specifies, in addition, the metric of the level below the clock. The beat-tone-cluster envelope concept was proposed to specify the figural part.},
issn={1532-5962},
doi={10.3758/BF03213077},
url={https://doi.org/10.3758/BF03213077}
}

@article{shmulevich2000perceptual,
author = { I.   Shmulevich  and  D.-J.   Povel },
title = {Measures of Temporal Pattern Complexity},
journal = {Journal of New Music Research},
volume = {29},
number = {1},
pages = {61-69},
year  = {2000},
publisher = {Routledge},
doi = {10.1076/0929-8215(200003)29:01;1-P;FT061},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1076/0929-8215%28200003%2929%3A01%3B1-P%3BFT061
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1076/0929-8215%28200003%2929%3A01%3B1-P%3BFT061
    
}

}

@article{mullensiefen2014gmsi,
    doi = {10.1371/journal.pone.0089642},
    author = {Müllensiefen, Daniel AND Gingras, Bruno AND Musil, Jason AND Stewart, Lauren},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {The Musicality of Non-Musicians: An Index for Assessing Musical Sophistication in the General Population},
    year = {2014},
    month = {02},
    volume = {9},
    url = {https://doi.org/10.1371/journal.pone.0089642},
    pages = {1-23},
    abstract = {Musical skills and expertise vary greatly in Western societies. Individuals can differ in their repertoire of musical behaviours as well as in the level of skill they display for any single musical behaviour. The types of musical behaviours we refer to here are broad, ranging from performance on an instrument and listening expertise, to the ability to employ music in functional settings or to communicate about music. In this paper, we first describe the concept of ‘musical sophistication’ which can be used to describe the multi-faceted nature of musical expertise. Next, we develop a novel measurement instrument, the Goldsmiths Musical Sophistication Index (Gold-MSI) to assess self-reported musical skills and behaviours on multiple dimensions in the general population using a large Internet sample (n = 147,636). Thirdly, we report results from several lab studies, demonstrating that the Gold-MSI possesses good psychometric properties, and that self-reported musical sophistication is associated with performance on two listening tasks. Finally, we identify occupation, occupational status, age, gender, and wealth as the main socio-demographic factors associated with musical sophistication. Results are discussed in terms of theoretical accounts of implicit and statistical music learning and with regard to social conditions of sophisticated musical engagement.},
    number = {2},

}

@inproceedings{wimmer2019usb,
author = {Wimmer, Raphael and Schmid, Andreas and Bockes, Florian},
title = {On the Latency of USB-Connected Input Devices},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300650},
doi = {10.1145/3290605.3300650},
abstract = {We propose a method for accurately and precisely measuring the intrinsic latency of input devices and document measurements for 36 keyboards, mice and gamepads connected via USB. Our research shows that devices differ not only in average latency, but also in the distribution of their latencies, and that forced polling at 1000 Hz decreases latency for some but not all devices. Existing practices - measuring end-to-end latency as a proxy of input latency and reporting only mean values and standard deviations - hide these characteristic latency distributions caused by device intrinsics and polling rates. A probabilistic model of input device latency demonstrates these issues and matches our measurements. Thus, our work offers guidance for researchers, engineers, and hobbyists who want to measure the latency of input devices or select devices with low latency.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {usb, input devices, lag, model, input, latency},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@Article{plant2009precision,
author={Plant, Richard R.
and Turner, Garry},
title={Millisecond precision psychological research in a world of commodity computers: New hardware, new problems?},
journal={Behavior Research Methods},
year={2009},
month={Aug},
day={01},
volume={41},
number={3},
pages={598-614},
abstract={Since the publication of Plant, Hammond, and Turner (2004), which highlighted a pressing need for researchers to pay more attention to sources of error in computer-based experiments, the landscape has undoubtedly changed, but not necessarily for the better. Readily available hardware has improved in terms of raw speed; multicore processors abound; graphics cards now have hundreds of megabytes of RAM; main memory is measured in gigabytes; drive space is measured in terabytes; ever larger thin film transistor displays capable of single-digit response times, together with newer Digital Light Processing multimedia projectors, enable much greater graphic complexity; and new 64-bit operating systems, such as Microsoft Vista, are now commonplace. However, have millisecond-accurate presentation and response timing improved, and will they ever be available in commodity computers and peripherals? In the present article, we used a Black Box ToolKit to measure the variability in timing characteristics of hardware used commonly in psychological research.},
issn={1554-3528},
doi={10.3758/BRM.41.3.598},
url={https://doi.org/10.3758/BRM.41.3.598}
}

@article{vannoorden1999resonance,
author = { Leon   van Noorden  and  Dirk   Moelants },
title = {Resonance in the Perception of Musical Pulse},
journal = {Journal of New Music Research},
volume = {28},
number = {1},
pages = {43-66},
year  = {1999},
publisher = {Routledge},
doi = {10.1076/jnmr.28.1.43.3122},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1076/jnmr.28.1.43.3122
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1076/jnmr.28.1.43.3122
}
}

@article{polak2010rhythmic,
  title={Rhythmic feel as meter: Non-isochronous beat subdivision in jembe
         music from Mali},
  author={Polak, Rainer},
  journal={Music Theory Online},
  volume={16},
  number={4},
  year={2010}
}

@article{johansson2017meter,
    author = {Johansson, Mats},
    title = "{Non-Isochronous Musical Meters: Towards a Multidimensional Model}",
    journal = {Ethnomusicology},
    volume = {61},
    number = {1},
    pages = {31-51},
    year = {2017},
    month = {01},
    abstract = "{This article examines different concepts and models of musical meter in light of a case that illustrates some of the challenges of developing a general theory. More precisely, in the musical example analyzed—a Swedish Polska tune—there seemingly are no isochronous metric levels (measure, beat, subdivision). Given this observation, the aim is to consider the explanatory potential of different models of metrical coherence as well as to suggest some new avenues of enquiry. The article concludes by offering an alternative framework for understanding how temporal relationships can be controlled and form coherent patterns despite vast irregularities.}",
    issn = {0014-1836},
    doi = {10.5406/ethnomusicology.61.1.0031},
    url = {https://doi.org/10.5406/ethnomusicology.61.1.0031},
    eprint = {https://scholarlypublishingcollective.org/uip/etm/article-pdf/61/1/31/1206305/ethnomusicology.61.1.0031.pdf},
}

@misc{website:mirex2019beat,
    title = "2009 Beat Tracking results",
    organization = "MIREX",
    url = "https://www.music-ir.org/mirex/wiki/2019:MIREX2019_Results"
}

@book{goodfellow2016deep,
	title={Deep Learning},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	note={\url{http://www.deeplearningbook.org}},
	year={2016}
}

@book{norvig2002modern,
  title={A modern approach},
  author={Norvig, P Russel and Intelligence, S Artificial},
  year={2002},
  publisher={Prentice Hall Upper Saddle River, NJ, USA:}
}


@article{wacongne2011hierarchy,
author = {Catherine Wacongne  and Etienne Labyt  and Virginie van Wassenhove  and Tristan Bekinschtein  and Lionel Naccache  and Stanislas Dehaene },
title = {Evidence for a hierarchy of predictions and prediction errors in human cortex},
journal = {Proceedings of the National Academy of Sciences},
volume = {108},
number = {51},
pages = {20754-20759},
year = {2011},
doi = {10.1073/pnas.1117807108},

URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1117807108},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1117807108}
,
    abstract = { According to hierarchical predictive coding models, the cortex constantly generates predictions of incoming stimuli at multiple levels of processing. Responses to auditory mismatches and omissions are interpreted as reflecting the prediction error when these predictions are violated. An alternative interpretation, however, is that neurons passively adapt to repeated stimuli. We separated these alternative interpretations by designing a hierarchical auditory novelty paradigm and recording human EEG and magnetoencephalographic (MEG) responses to mismatching or omitted stimuli. In the crucial condition, participants listened to frequent series of four identical tones followed by a fifth different tone, which generates a mismatch response. Because this response itself is frequent and expected, the hierarchical predictive coding hypothesis suggests that it should be cancelled out by a higher-order prediction. Three consequences ensue. First, the mismatch response should be larger when it is unexpected than when it is expected. Second, a perfectly monotonic sequence of five identical tones should now elicit a higher-order novelty response. Third, omitting the fifth tone should reveal the brain's hierarchical predictions. The rationale here is that, when a deviant tone is expected, its omission represents a violation of two expectations: a local prediction of a tone plus a hierarchically higher expectation of its deviancy. Thus, such an omission should induce a greater prediction error than when a standard tone is expected. Simultaneous EEE- magnetoencephalographic recordings verify those predictions and thus strongly support the predictive coding hypothesis. Higher-order predictions appear to be generated in multiple areas of frontal and associative cortices. }
}

@book{kennedy2013oxford,
  title={The Oxford dictionary of music},
  author={Kennedy, Michael and Kennedy, Joyce},
  year={2013},
  publisher={Oxford University Press}
}

@article{higgins1984monophonic,
    author = {Longuet-Higgins, H. C. and Lee, C. S.},
    title = "{The Rhythmic Interpretation of Monophonic Music}",
    journal = {Music Perception},
    volume = {1},
    number = {4},
    pages = {424-441},
    year = {1984},
    month = {07},
    abstract = "{The assignment of a rhythmic interpretation to a piece of metrical music calls for the postulation of an underlying meter and the parsing of the note values according to this meter. In this article we develop the implications of this view, which include the following propositions. 1. Any given sequence of note values is in principle rhythmically ambiguous, although this ambiguity is seldom apparent to the listener. 2. In choosing a rhythmic interpretation for a given note sequence the listener seems to be guided by a strong assumption: if the sequence can be interpreted as the realization of an unsyncopated passage, then that is how he will interpret it. 3. Phrasing can make an important difference to the rhythmic interpretation that the listener assigns to a given sequence. Phrasing can therefore serve a structural function as well as a purely ornamental one.}",
    issn = {0730-7829},
    doi = {10.2307/40285271},
    url = {https://doi.org/10.2307/40285271},
    eprint = {https://online.ucpress.edu/mp/article-pdf/1/4/424/145374/40285271.pdf},
}

@article{keith1991polychords,
  title={From polychords to polya: adventures in musical combinatorics},
  author={Keith, Michael},
  year={1991},
  publisher={Vinculum Press}
}

@article{hurley2014spontaneous,
  title={Spontaneous sensorimotor coupling with multipart music.},
  author={Hurley, Brian K and Martens, Peter A and Janata, Petr},
  journal={Journal of Experimental Psychology: Human Perception and Performance},
  volume={40},
  number={4},
  pages={1679},
  year={2014},
  publisher={American Psychological Association}
}

@InCollection{scaratino2021emotion,
	author       =	{Scarantino, Andrea and de Sousa, Ronald},
	title        =	{{Emotion}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/sum2021/entries/emotion/}},
	year         =	{2021},
	edition      =	{{S}ummer 2021},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@article{russell1980circumplex,
  title={A circumplex model of affect.},
  author={Russell, James A},
  journal={Journal of personality and social psychology},
  volume={39},
  number={6},
  pages={1161},
  year={1980},
  publisher={American Psychological Association}
}

@article{zentner2008emotions,
  title={Emotions evoked by the sound of music: characterization, classification, and measurement.},
  author={Zentner, Marcel and Grandjean, Didier and Scherer, Klaus R},
  journal={Emotion},
  volume={8},
  number={4},
  pages={494},
  year={2008},
  publisher={American Psychological Association}
}

@article{pearce2018enculturation,
author = {Pearce, Marcus T.},
title = {Statistical learning and probabilistic prediction in music cognition: mechanisms of stylistic enculturation},
journal = {Annals of the New York Academy of Sciences},
volume = {1423},
number = {1},
pages = {378-395},
keywords = {music perception, enculturation, statistical learning, probabilistic prediction, IDyOM},
doi = {https://doi.org/10.1111/nyas.13654},
url = {https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/nyas.13654},
eprint = {https://nyaspubs.onlinelibrary.wiley.com/doi/pdf/10.1111/nyas.13654},
abstract = {Abstract Music perception depends on internal psychological models derived through exposure to a musical culture. It is hypothesized that this musical enculturation depends on two cognitive processes: (1) statistical learning, in which listeners acquire internal cognitive models of statistical regularities present in the music to which they are exposed; and (2) probabilistic prediction based on these learned models that enables listeners to organize and process their mental representations of music. To corroborate these hypotheses, I review research that uses a computational model of probabilistic prediction based on statistical learning (the information dynamics of music (IDyOM) model) to simulate data from empirical studies of human listeners. The results show that a broad range of psychological processes involved in music perception—expectation, emotion, memory, similarity, segmentation, and meter—can be understood in terms of a single, underlying process of probabilistic prediction using learned statistical models. Furthermore, IDyOM simulations of listeners from different musical cultures demonstrate that statistical learning can plausibly predict causal effects of differential cultural exposure to musical styles, providing a quantitative model of cultural distance. Understanding the neural basis of musical enculturation will benefit from close coordination between empirical neuroimaging and computational modeling of underlying mechanisms, as outlined here.},
year = {2018}
}

@article{grahn2009premotor,
	author = {Grahn, Jessica A. and Rowe, James B.},
	title = {Feeling the Beat: Premotor and Striatal Interactions in Musicians and Nonmusicians during Beat Perception},
	volume = {29},
	number = {23},
	pages = {7540--7548},
	year = {2009},
	doi = {10.1523/JNEUROSCI.2018-08.2009},
	publisher = {Society for Neuroscience},
	abstract = {Little is known about the underlying neurobiology of rhythm and beat perception, despite its universal cultural importance. Here we used functional magnetic resonance imaging to study rhythm perception in musicians and nonmusicians. Three conditions varied in the degree to which external reinforcement versus internal generation of the beat was required. The {\textquotedblleft}volume{\textquotedblright} condition strongly externally marked the beat with volume changes, the {\textquotedblleft}duration{\textquotedblright} condition marked the beat with weaker accents arising from duration changes, and the {\textquotedblleft}unaccented{\textquotedblright} condition required the beat to be entirely internally generated. In all conditions, beat rhythms compared with nonbeat control rhythms revealed putamen activity. The presence of a beat was also associated with greater connectivity between the putamen and the supplementary motor area (SMA), the premotor cortex (PMC), and auditory cortex. In contrast, the type of accent within the beat conditions modulated the coupling between premotor and auditory cortex, with greater modulation for musicians than nonmusicians. Importantly, the response of the putamen to beat conditions was not attributable to differences in temporal complexity between the three rhythm conditions. We propose that a cortico-subcortical network including the putamen, SMA, and PMC is engaged for the analysis of temporal sequences and prediction or generation of putative beats, especially under conditions that may require internal generation of the beat. The importance of this system for auditory{\textendash}motor interaction and development of precisely timed movement is suggested here by its facilitation in musicians.},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/29/23/7540},
	eprint = {https://www.jneurosci.org/content/29/23/7540.full.pdf},
	journal = {Journal of Neuroscience}
}

@book{gabis2006armonia,
  title={Armon{\'\i}a funcional},
  author={Gabis, Claudio},
  year={2006},
  publisher={Melos Ediciones Musicales}
}

<<<<<<< HEAD
@article{hansen2016bebop,
    doi = {10.1371/journal.pone.0163584},
    author = {Hansen, Niels Chr. AND Vuust, Peter AND Pearce, Marcus},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {"If You Have to Ask, You'll Never Know": Effects of Specialised Stylistic Expertise on Predictive Processing of Music},
    year = {2016},
    month = {10},
    volume = {11},
    url = {https://doi.org/10.1371/journal.pone.0163584},
    pages = {1-20},
    abstract = {Musical expertise entails meticulous stylistic specialisation and enculturation. Even so, research on musical training effects has focused on generalised comparisons between musicians and non-musicians, and cross-cultural work addressing specialised expertise has traded cultural specificity and sensitivity for other methodological limitations. This study aimed to experimentally dissociate the effects of specialised stylistic training and general musical expertise on the perception of melodies. Non-musicians and professional musicians specialising in classical music or jazz listened to sampled renditions of saxophone solos improvised by Charlie Parker in the bebop style. Ratings of explicit uncertainty and expectedness for different continuations of each melodic excerpt were collected. An information-theoretic model of expectation enabled selection of stimuli affording highly certain continuations in the bebop style, but highly uncertain continuations in the context of general tonal expectations, and vice versa. The results showed that expert musicians have acquired probabilistic characteristics of music influencing their experience of expectedness and predictive uncertainty. While classical musicians had internalised key aspects of the bebop style implicitly, only jazz musicians’ explicit uncertainty ratings reflected the computational estimates, and jazz-specific expertise modulated the relationship between explicit and inferred uncertainty data. In spite of this, there was no evidence that non-musicians and classical musicians used a stylistically irrelevant cognitive model of general tonal music providing support for the theory of cognitive firewalls between stylistic models in predictive processing of music.},
    number = {10},

}

@ARTICLE{hansen2014uncertainty,
    AUTHOR={Hansen, Niels Chr. and Pearce, Marcus T.},   
    TITLE={Predictive uncertainty in auditory sequence processing},      
    JOURNAL={Frontiers in Psychology},      
    VOLUME={5},           
    YEAR={2014},      
    URL={https://www.frontiersin.org/articles/10.3389/fpsyg.2014.01052},       
    DOI={10.3389/fpsyg.2014.01052},      
    ISSN={1664-1078},   
    ABSTRACT={Previous studies of auditory expectation have focused on the expectedness perceived by listeners retrospectively in response to events. In contrast, this research examines predictive uncertainty—a property of listeners' prospective state of expectation prior to the onset of an event. We examine the information-theoretic concept of Shannon entropy as a model of predictive uncertainty in music cognition. This is motivated by the Statistical Learning Hypothesis, which proposes that schematic expectations reflect probabilistic relationships between sensory events learned implicitly through exposure. Using probability estimates from an unsupervised, variable-order Markov model, 12 melodic contexts high in entropy and 12 melodic contexts low in entropy were selected from two musical repertoires differing in structural complexity (simple and complex). Musicians and non-musicians listened to the stimuli and provided explicit judgments of perceived uncertainty (explicit uncertainty). We also examined an indirect measure of uncertainty computed as the entropy of expectedness distributions obtained using a classical probe-tone paradigm where listeners rated the perceived expectedness of the final note in a melodic sequence (inferred uncertainty). Finally, we simulate listeners' perception of expectedness and uncertainty using computational models of auditory expectation. A detailed model comparison indicates which model parameters maximize fit to the data and how they compare to existing models in the literature. The results show that listeners experience greater uncertainty in high-entropy musical contexts than low-entropy contexts. This effect is particularly apparent for inferred uncertainty and is stronger in musicians than non-musicians. Consistent with the Statistical Learning Hypothesis, the results suggest that increased domain-relevant training is associated with an increasingly accurate cognitive model of probabilistic structure in music.}
}

@book{briot2020deep,
  title={Deep learning techniques for music generation},
  author={Briot, Jean-Pierre and Hadjeres, Ga{\"e}tan and Pachet, Fran{\c{c}}ois-David},
  volume={1},
  year={2020},
  publisher={Springer}
}

@inproceedings{downie2008mood,
  title={The 2007 MIREX audio mood classification task: Lessons learned},
  author={Downie, XHJS and Laurier, Cyril and Ehmann, MBAF},
  booktitle={Proc. 9th Int. Conf. Music Inf. Retrieval},
  pages={462--467},
  year={2008}
}

@article{mcfee2017evaluating,
  title={Evaluating hierarchical structure in music annotations},
  author={McFee, Brian and Nieto, Oriol and Farbood, Morwaread M and Bello, Juan Pablo},
  journal={Frontiers in psychology},
  volume={8},
  pages={1337},
  year={2017},
  publisher={Frontiers Media SA}
}

@article{huang2018music,
    title={Music transformer},
    author={Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob
    and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai,
    Andrew M and Hoffman, Matthew D and Dinculescu, Monica and Eck,
    Douglas},
    journal={arXiv preprint arXiv:1809.04281},
    year={2018}
}

@article{zhang2020using,
  title={Using pupillometry to investigate predictive processes in infancy},
  author={Zhang, Felicia and Emberson, Lauren L},
  journal={Infancy},
  volume={25},
  number={6},
  pages={758--780},
  year={2020},
  publisher={Wiley Online Library}
}

@article{bujia2022modeling,
    title={Modeling Human Visual Search in Natural Scenes: A Combined
    Bayesian Searcher and Saliency Map Approach},
    author={Bujia, Gaston and Sclar, Melanie and Vita, Sebastian and Solovey,
    Guillermo and Kamienkowski, Juan Esteban},
    journal={Frontiers in systems neuroscience},
    volume={16},
    year={2022},
    publisher={Frontiers Media SA}
}

@Article{bedi2015psychosis,
author={Bedi, Gillinder
and Carrillo, Facundo
and Cecchi, Guillermo A.
and Slezak, Diego Fern{\'a}ndez
and Sigman, Mariano
and Mota, Nat{\'a}lia B.
and Ribeiro, Sidarta
and Javitt, Daniel C.
and Copelli, Mauro
and Corcoran, Cheryl M.},
title={Automated analysis of free speech predicts psychosis onset in high-risk youths},
journal={npj Schizophrenia},
year={2015},
month={Aug},
day={26},
volume={1},
number={1},
pages={15030},
abstract={Psychiatry lacks the objective clinical tests routinely used in other specializations. Novel computerized methods to characterize complex behaviors such as speech could be used to identify and predict psychiatric illness in individuals.},
issn={2334-265X},
doi={10.1038/npjschz.2015.30},
url={https://doi.org/10.1038/npjschz.2015.30}
}


@Article{bianchi2020predictability,
author={Bianchi, Bruno
and Bengolea Monz{\'o}n, Gast{\'o}n
and Ferrer, Luciana
and Fern{\'a}ndez Slezak, Diego
and Shalom, Diego E.
and Kamienkowski, Juan E.},
title={Human and computer estimations of Predictability of words in written language},
journal={Scientific Reports},
year={2020},
month={Mar},
day={10},
volume={10},
number={1},
pages={4396},
abstract={When we read printed text, we are continuously predicting upcoming words to integrate information and guide future eye movements. Thus, the Predictability of a given word has become one of the most important variables when explaining human behaviour and information processing during reading. In parallel, the Natural Language Processing (NLP) field evolved by developing a wide variety of applications. Here, we show that using different word embeddings techniques (like Latent Semantic Analysis, Word2Vec, and FastText) and N-gram-based language models we were able to estimate how humans predict words (cloze-task Predictability) and how to better understand eye movements in long Spanish texts. Both types of models partially captured aspects of predictability. On the one hand, our N-gram model performed well when added as a replacement for the cloze-task Predictability of the fixated word. On the other hand, word embeddings were useful to mimic Predictability of the following word. Our study joins efforts from neurolinguistic and NLP fields to understand human information processing during reading to potentially improve NLP algorithms.},
issn={2045-2322},
doi={10.1038/s41598-020-61353-z},
url={https://doi.org/10.1038/s41598-020-61353-z}
}


